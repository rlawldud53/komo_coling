{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import time\n","\n","current_time = time.time()\n","local_time = time.localtime(current_time)\n","formatted_time = time.strftime(\"%m%d_%H%M\", local_time)"]},{"cell_type":"markdown","metadata":{"id":"77urXhqnARGK"},"source":["# Setup\n"]},{"cell_type":"markdown","metadata":{"id":"QcVVwa7-ug7J"},"source":["## Install Libs"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80252,"status":"ok","timestamp":1716819462026,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"ggOzyY64FApW","outputId":"357f9a8b-bfe0-490c-d42b-eb846359041e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: torch 1.13.1+cu116\n","Uninstalling torch-1.13.1+cu116:\n","  Successfully uninstalled torch-1.13.1+cu116\n","Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n","Collecting torch==1.13.1+cu116\n","  Using cached https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp310-cp310-linux_x86_64.whl (1977.9 MB)\n","Requirement already satisfied: typing-extensions in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from torch==1.13.1+cu116) (4.11.0)\n","Installing collected packages: torch\n","Successfully installed torch-1.13.1+cu116\n"]}],"source":["downgrade torch for colossalai\n","!pip uninstall torch -y\n","!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":80141,"status":"ok","timestamp":1716819542164,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"CpWnOamUAT6E","outputId":"a8eb10d8-7e1f-436e-cca1-76da589734dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==4.35.2\n","  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n","Requirement already satisfied: filelock in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (0.23.2)\n","Requirement already satisfied: numpy>=1.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (2024.5.15)\n","Requirement already satisfied: requests in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (2.32.2)\n","Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n","  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers==4.35.2) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2024.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->transformers==4.35.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->transformers==4.35.2) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->transformers==4.35.2) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->transformers==4.35.2) (2024.2.2)\n","Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n","Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.3\n","    Uninstalling tokenizers-0.13.3:\n","      Successfully uninstalled tokenizers-0.13.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.26.1\n","    Uninstalling transformers-4.26.1:\n","      Successfully uninstalled transformers-4.26.1\n","Successfully installed tokenizers-0.15.2 transformers-4.35.2\n","Collecting accelerate==0.24.1\n","  Using cached accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: numpy>=1.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from accelerate==0.24.1) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from accelerate==0.24.1) (23.2)\n","Requirement already satisfied: psutil in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from accelerate==0.24.1) (5.9.8)\n","Requirement already satisfied: pyyaml in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from accelerate==0.24.1) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from accelerate==0.24.1) (1.13.1+cu116)\n","Requirement already satisfied: huggingface-hub in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from accelerate==0.24.1) (0.23.2)\n","Requirement already satisfied: typing-extensions in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.24.1) (4.11.0)\n","Requirement already satisfied: filelock in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.24.1) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.24.1) (2024.3.1)\n","Requirement already satisfied: requests in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.24.1) (2.32.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.24.1) (4.66.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.24.1) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.24.1) (2024.2.2)\n","Using cached accelerate-0.24.1-py3-none-any.whl (261 kB)\n","Installing collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.31.0.dev0\n","    Uninstalling accelerate-0.31.0.dev0:\n","      Successfully uninstalled accelerate-0.31.0.dev0\n","Successfully installed accelerate-0.24.1\n","fatal: destination path 'KoChatGPT' already exists and is not an empty directory.\n","mv: cannot move 'KoChatGPT/data_kochatgpt' to './data_kochatgpt': Directory not empty\n","mv: cannot move 'KoChatGPT/img' to './img': Directory not empty\n","/home/phw/work/mkj/KoChatGPT/colossalai_ChatGPT_230319\n","Processing /home/phw/work/mkj/KoChatGPT/colossalai_ChatGPT_230319\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: transformers>=4.20.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from chatgpt==0.1.0) (4.35.2)\n","Requirement already satisfied: tqdm in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from chatgpt==0.1.0) (4.66.4)\n","Requirement already satisfied: datasets in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from chatgpt==0.1.0) (2.8.0)\n","Requirement already satisfied: loralib in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from chatgpt==0.1.0) (0.1.2)\n","Requirement already satisfied: colossalai>=0.2.4 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from chatgpt==0.1.0) (0.2.7)\n","Requirement already satisfied: torch in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from chatgpt==0.1.0) (1.13.1+cu116)\n","Requirement already satisfied: langchain in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from chatgpt==0.1.0) (0.0.113)\n","Requirement already satisfied: numpy in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.26.4)\n","Requirement already satisfied: psutil in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.8)\n","Requirement already satisfied: packaging in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.2)\n","Requirement already satisfied: pre-commit in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.7.1)\n","Requirement already satisfied: rich in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.7.1)\n","Requirement already satisfied: click in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.7)\n","Requirement already satisfied: fabric in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.2.2)\n","Requirement already satisfied: contexttimer in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n","Requirement already satisfied: ninja in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1.1)\n","Requirement already satisfied: filelock in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2024.5.15)\n","Requirement already satisfied: requests in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.32.2)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.4.3)\n","Requirement already satisfied: pyarrow>=6.0.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from datasets->chatgpt==0.1.0) (16.1.0)\n","Requirement already satisfied: dill<0.3.7 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from datasets->chatgpt==0.1.0) (0.3.6)\n","Requirement already satisfied: pandas in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from datasets->chatgpt==0.1.0) (2.2.2)\n","Requirement already satisfied: xxhash in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from datasets->chatgpt==0.1.0) (3.4.1)\n","Requirement already satisfied: multiprocess in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from datasets->chatgpt==0.1.0) (0.70.14)\n","Requirement already satisfied: fsspec>=2021.11.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets->chatgpt==0.1.0) (2024.3.1)\n","Requirement already satisfied: aiohttp in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from datasets->chatgpt==0.1.0) (3.9.5)\n","Requirement already satisfied: responses<0.19 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from datasets->chatgpt==0.1.0) (0.18.0)\n","Requirement already satisfied: SQLAlchemy<2,>=1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain->chatgpt==0.1.0) (1.4.52)\n","Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain->chatgpt==0.1.0) (0.5.14)\n","Requirement already satisfied: pydantic<2,>=1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain->chatgpt==0.1.0) (1.10.15)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain->chatgpt==0.1.0) (8.3.0)\n","Requirement already satisfied: typing-extensions in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from torch->chatgpt==0.1.0) (4.11.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.21.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (3.0.3)\n","Requirement already satisfied: invoke>=2.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n","Requirement already satisfied: paramiko>=2.4 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.4.0)\n","Requirement already satisfied: decorator>=5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (5.1.1)\n","Requirement already satisfied: deprecated>=1.2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.2.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from pandas->datasets->chatgpt==0.1.0) (2.9.0)\n","Requirement already satisfied: pytz>=2020.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from pandas->datasets->chatgpt==0.1.0) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from pandas->datasets->chatgpt==0.1.0) (2024.1)\n","Requirement already satisfied: cfgv>=2.0.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.4.0)\n","Requirement already satisfied: identify>=1.0.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.36)\n","Requirement already satisfied: nodeenv>=0.11.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n","Requirement already satisfied: virtualenv>=20.10.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.26.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.18.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from deprecated>=1.2->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n","Requirement already satisfied: setuptools in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (69.5.1)\n","Requirement already satisfied: bcrypt>=3.2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.1.3)\n","Requirement already satisfied: cryptography>=3.3 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (42.0.7)\n","Requirement already satisfied: pynacl>=1.5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n","Requirement already satisfied: six>=1.5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.0.0)\n","Requirement already satisfied: distlib<1,>=0.3.7 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.8)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (4.2.2)\n","Requirement already satisfied: cffi>=1.12 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.16.0)\n","Requirement already satisfied: pycparser in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.22)\n","Building wheels for collected packages: chatgpt\n","  Building wheel for chatgpt (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46645 sha256=bbde19365ae180481c33360d5027b79c1920ba0ffb5206abd9ad0dac5c4ab020\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-lk1wxmg3/wheels/0f/b9/e9/205dc094aa3c1a24bf7e6ade74fb308153284468d7965d4336\n","Successfully built chatgpt\n","Installing collected packages: chatgpt\n","  Attempting uninstall: chatgpt\n","    Found existing installation: chatgpt 0.1.0\n","    Uninstalling chatgpt-0.1.0:\n","      Successfully uninstalled chatgpt-0.1.0\n","Successfully installed chatgpt-0.1.0\n","/home/phw/work/mkj\n","Requirement already satisfied: openai in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (1.30.3)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from openai) (4.4.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from openai) (0.27.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from openai) (1.10.15)\n","Requirement already satisfied: sniffio in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from openai) (4.11.0)\n","Requirement already satisfied: idna>=2.8 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n","Requirement already satisfied: certifi in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: langchain==0.0.113 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (0.0.113)\n","Requirement already satisfied: PyYAML<7,>=6 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain==0.0.113) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<2,>=1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain==0.0.113) (1.4.52)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain==0.0.113) (3.9.5)\n","Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain==0.0.113) (0.5.14)\n","Requirement already satisfied: numpy<2,>=1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain==0.0.113) (1.26.4)\n","Requirement already satisfied: pydantic<2,>=1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain==0.0.113) (1.10.15)\n","Requirement already satisfied: requests<3,>=2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain==0.0.113) (2.32.2)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from langchain==0.0.113) (8.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.21.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.9.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from pydantic<2,>=1->langchain==0.0.113) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.113) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (3.0.3)\n","Requirement already satisfied: packaging>=17.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n"]}],"source":["import torch\n","\n","!pip install transformers==4.35.2\n","!pip install accelerate==0.24.1\n","\n","# for ColossalAI\n","!pip install -q colossalai==0.2.7\n","\n","# setup data\n","!git clone https://github.com/airobotlab/KoChatGPT\n","!mv KoChatGPT/data_kochatgpt .\n","!mv KoChatGPT/img .\n","\n","%cd KoChatGPT/colossalai_ChatGPT_230319/\n","!pip install .\n","%cd ../../\n","\n","# setup library\n","!pip install openai\n","!pip install langchain==0.0.113\n","!pip install pandas>=1.4.1"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":50795,"status":"ok","timestamp":1716819592955,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"Dez6uL9_BCIB","outputId":"b3b0ccb0-21d8-453e-eeef-b7489e81943f"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# for Koalpaca\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U transformers==4.26.1\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q datasets==2.8.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5314,"status":"ok","timestamp":1716819598266,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"lIQOmEAdBWbm","outputId":"763650f2-0e8b-4f53-852b-909f060d2eb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: loralib in /home/phw/miniconda3/envs/llama/lib/python3.10/site-packages (0.1.2)\n"]}],"source":["# !pip install loralib"]},{"cell_type":"markdown","metadata":{"id":"iP9oiTBLueTz"},"source":["## Import"]},{"cell_type":"markdown","metadata":{"id":"aQyi9YyGOrMs"},"source":["/usr/local/lib/python3.10/dist-packages/chatgpt/trainer/rm.py\n","/usr/local/lib/python3.10/dist-packages/chatgpt/models/base/reward_model.py"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":6505,"status":"ok","timestamp":1716823144415,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"0CpKDA95AmF1","outputId":"667d11fc-fd0c-4635-dec3-2501def22c61"},"outputs":[],"source":["# for general, SFT\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from datasets import load_dataset\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n","from transformers import Trainer, TrainingArguments, AutoModelWithLMHead, BloomTokenizerFast\n","from peft import PeftModel, PeftConfig\n","from copy import deepcopy\n","from torch.optim import Adam\n","import pandas as pd\n","import argparse\n","import copy\n","import logging\n","import json\n","from dataclasses import dataclass, field\n","import argparse\n","\n","# for RM\n","import loralib as lora\n","torch.cuda.empty_cache()\n","from chatgpt.dataset import RewardDataset\n","from chatgpt.models.base import RewardModel\n","\n","from chatgpt.models.bloom import BLOOMRM\n","from chatgpt.models.gpt import GPTRM\n","from chatgpt.models.opt import OPTRM\n","from chatgpt.trainer import RewardModelTrainer\n","from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","\n","from colossalai.nn.optimizer import HybridAdam\n","\n","# for PPO\n","from copy import deepcopy\n","\n","from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n","from chatgpt.models.gpt import GPTActor, GPTCritic\n","from chatgpt.models.opt import OPTActor, OPTCritic\n","from chatgpt.trainer import PPOTrainer\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","\n","def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n","    \"\"\"Collects the state dict and dump to disk.\"\"\"\n","    state_dict = trainer.model.state_dict()\n","    if trainer.args.should_save:\n","        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n","        del state_dict\n","        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"]},{"cell_type":"markdown","metadata":{"id":"pcdaANRQyBvY"},"source":["## Define the path"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716823146770,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"NmmvO2BBKir2"},"outputs":[],"source":["#dataset\n","DATA_PATH = '/content/drive/MyDrive/KoMo/Dataset/total_minus_38000.csv'\n","RM_DATASET_PATH = 'dataset/dataset_38000_rang.json'\n","# PPO_DATASET_PATH= 'dataset/PPO_38000.json' # 직접 만든 dataset\n","# PPO_DATASET_PATH2 = 'dataset/PPO_profanity_sentences.json'   # Unsmile 욕설 데이터셋\n","PPO_DATASET_PATH = 'dataset/PPO_extra_36068.json'\n","\n","#SFT\n","# Alpaca_SFT_MODEL_PATH = \"canho/koalpaca-5.8b-5epochs-0524\"    # 예전 koalpaca\n","# Alpaca_SFT_MODEL_PATH = \"canho/koalpaca-5.8b-emojis-5epochs-final\"  # 이모지 가득 prompt finetuning한 koalpaca\n","Alpaca_SFT_MODEL_PATH = \"canho/koalpaca-5.8b-emojis-3epochs-prompt-revised\"  # 이모지 적정히 prompt finetuning한 koalpaca\n","KoGPT_SFT_MODEL_PATH = 'SFT/0521_2/checkpoint-2500'\n","\n","#RM+PPO\n","RM_MODEL_PATH = 'RM/0529_1648/RM_model_epoch2_0.06965.pt'\n","# RM_MODEL_PATH = 'mingming2000/RM_koAlpaca_MLP2_20000_2epoch'\n","RM_OPTIM_PATH = 'RM/0529_1648/RM_optim_epoch2_0.06965.pt'\n","# RM_OPTIM_PATH = 'mingming2000/RM_koAlpaca_MLP2_20000_2epoch'\n","# RM_PATH = 'RM/0529_1648/'\n","RM_PATH = 'mingming2000/RM_koAlpaca_MLP2_20000_2epoch'\n","\n","RM_MODEL_OUTPUT_PATH = f'RM/'\n","# RM_MODEL_OUTPUT_PATH = f'RM/{formatted_time}'\n","PPO_MODEL_OUTPUT_PATH = f'PPO/{formatted_time}/'\n","# PPO_MODEL_OUTPUT_PATH = f'PPO/'"]},{"cell_type":"markdown","metadata":{"id":"Xk7yn72EMULt"},"source":["# SFT"]},{"cell_type":"markdown","metadata":{"id":"8s1p2SnEMXCq"},"source":["## KoGPT SFT"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTgCYv9shlBC","outputId":"1f059d1d-a11d-409f-b341-261daa072484"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/phw/miniconda3/envs/komo/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["Embedding(51201, 768)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# define argment\n","parser = argparse.ArgumentParser()\n","DATA_PATH = '/content/drive/MyDrive/KoMo/Dataset/total_minus.csv'\n","OUTPUT_PATH = '/content/drive/MyDrive/KoMo/SFT/SFT_0521'\n","parser.add_argument('--data_path_1_SFT', type=str, default=DATA_PATH)\n","parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n","parser.add_argument('--max_epochs', type=int, default=2)\n","parser.add_argument('--train_batch_size', type=int, default=8)\n","parser.add_argument('--output_dir', type=str, default=OUTPUT_PATH)\n","\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n","args.max_epochs = 5\n","\n","# load model\n","model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n","tokenizer = transformers.AutoTokenizer.from_pretrained(\n","    args.model_name,\n","    padding_side=\"right\",\n","    model_max_length=512,\n",")\n","\n","# data config\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","\n","\n","tokenizer.add_special_tokens(\n","    {\n","        \"eos_token\": DEFAULT_EOS_TOKEN,\n","        \"bos_token\": DEFAULT_BOS_TOKEN,\n","        \"unk_token\": DEFAULT_UNK_TOKEN,\n","    }\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"39TS4FP5A-et"},"source":["## Koalpaca SFT"]},{"cell_type":"markdown","metadata":{"id":"fAlpHg2tXIex"},"source":["Load Koalpaca SFT model with checkpoint"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1491,"status":"ok","timestamp":1716823148259,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"NT3CdY2eBMMQ","outputId":"16fa4275-e9dd-452a-c4b1-fc7492ca70d7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/phw/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel, PeftConfig\n","\n","# peft_model_id = \"canho/koalpaca-5.8b-5epochs-0524\"  # 예전 koalpaca\n","# peft_model_id = \"canho/koalpaca-5.8b-emojis-5epochs-final\"\n","peft_model_id = Alpaca_SFT_MODEL_PATH\n","config = PeftConfig.from_pretrained(peft_model_id)\n","base_model_name = config.base_model_name_or_path\n","\n","# base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n","# model = PeftModel.from_pretrained(base_model, peft_model_id)\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# model.save_pretrained(\"ko\")"]},{"cell_type":"markdown","metadata":{"id":"51VEErpDzlDF"},"source":["## inference"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def inference_SFT(input_text='인공지능은 인공지능 입니다', print_log = True, device = 'cuda'):\n","\n","    if args.model == 'gpt2':\n","        input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","            torch.cuda.current_device())\n","        output = model(input_ids)\n","        output_reward = output.cpu().detach().numpy()[0]\n","\n","        if print_log:\n","            print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n","\n","        return output_reward\n","    \n","    elif args.model == 'alpaca':\n","        \n","        inputs = tokenizer(\n","            f\"instruction: input(순화할 문장): {input_text}\\n\\n output(순화된 문장) :\",\n","            return_tensors='pt',\n","            return_token_type_ids=False\n","        )\n","\n","        # 입력 텐서를 모델과 동일한 장치로 이동\n","        inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","        gened = model.generate(\n","            **inputs,\n","            max_new_tokens=128,\n","            early_stopping=True,\n","            do_sample=True,\n","            eos_token_id=2,\n","        )\n","        if print_log:            \n","            print(tokenizer.decode(gened[0]))\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"AhOrpJtcClym"},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m### Inference of SFT model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m(sent):\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model.eval()\n","\n","### Inference of SFT model\n","\n","def inference(sent):\n","    gened = model.generate(\n","        **tokenizer(\n","            f\"### 명령어: input에 욕설, 혐오 표현이 있다면 이를 찾아 예쁜 말로 순화해주세요. output은 하나의 문장으로만 출력해주세요.\\n\\n### 맥락: {sent}\\n\\n### 답변: \",\n","            return_tensors='pt',\n","            return_token_type_ids=False\n","        ),\n","        max_new_tokens=128,\n","        early_stopping=True,\n","        do_sample=True,\n","        eos_token_id=2,\n","    )\n","    print(tokenizer.decode(gened[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPiqjiQaUIgc"},"outputs":[],"source":["inference('미친놈들이 우글우글한 세상이구나 말세다 말세')\n","inference('알파카 야 이 씨발년아 말좀 쳐 들어')"]},{"cell_type":"markdown","metadata":{"id":"eyR9pnKsWzdW"},"source":["## Setting"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716823148260,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"Keq7z4bsBShT"},"outputs":[],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716823148260,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"CLPdjnnEz9EO","outputId":"ce39593e-3661-4551-b89a-1c012dc3199b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(output_dir='RM/', data_path_2_RM='dataset/dataset_38000_rang.json', strategy='naive', model='alpaca', pretrain='canho/koalpaca-5.8b-emojis-3epochs-prompt-revised', dataset='Dahoas/rm-static', save_path='rm_ckpt.pth', max_epochs=5, batch_size=4, lora_rank=0, max_len=64, verbose=True)\n"]}],"source":["# data config\n","import os\n","IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","os.environ['RANK'] = '0'\n","os.environ['LOCAL_RANK'] = '0'\n","os.environ['WORLD_SIZE'] = '2'\n","os.environ['MASTER_ADDR'] = '127.0.0.1'\n","\n","# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--output_dir', type=str, default=RM_MODEL_OUTPUT_PATH)\n","parser.add_argument('--data_path_2_RM', type=str, default=RM_DATASET_PATH)\n","parser.add_argument('--strategy',\n","                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n","                    default='naive')\n","parser.add_argument('--model', type=str, default='alpaca', choices=['gpt2', 'alpaca'])\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n","parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n","parser.add_argument('--max_epochs', type=int, default=10)\n","parser.add_argument('--batch_size', type=int, default=4)\n","parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n","parser.add_argument('--max_len', type=int, default=64)\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.max_epochs = 5\n","args.batch_size = 4\n","#args.pretrain = KoGPT_SFT_MODEL_PATH\n","args.pretrain = Alpaca_SFT_MODEL_PATH\n","args.verbose = True\n","\n","print(args)\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716823148260,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"UlHwWfGYWrbM"},"outputs":[],"source":["# configure strategy\n","if args.strategy == 'naive':\n","    strategy = NaiveStrategy()\n","elif args.strategy == 'ddp':\n","    strategy = DDPStrategy()\n","elif args.strategy == 'colossalai_gemini':\n","    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n","elif args.strategy == 'colossalai_zero2':\n","    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n","else:\n","    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"]},{"cell_type":"markdown","metadata":{"id":"IE4iB12XrnN4"},"source":["# RM Load"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":299,"status":"ok","timestamp":1716821015805,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"_s47Y1BHN_hm"},"outputs":[],"source":["# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n","from typing import Optional\n","\n","import torch.nn as nn\n","from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n","\n","# from ..base import RewardModel\n","from chatgpt.models.base import RewardModel\n","\n","\n","class GPTRM_custom_Koalpaca(RewardModel):\n","    \"\"\"\n","    GPT Reward model.\n","    Args:\n","        pretrained (str): Pretrained model name or path.\n","        config (GPT2Config): Model config.\n","        checkpoint (bool): Enable gradient checkpointing.\n","        lora_rank (int): Rank of the low-rank approximation.\n","        lora_train_bias (str): LoRA bias training mode.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 pretrained: Optional[str] = None,\n","                 config: Optional[GPT2Config] = None,\n","                 checkpoint: bool = False,\n","                 lora_rank: int = 0,\n","                 lora_train_bias: str = 'none',\n","                 tokenizer=None) -> None:\n","        \n","        if pretrained is not None:\n","            config = PeftConfig.from_pretrained(pretrained)\n","            base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n","            model = PeftModel.from_pretrained(base_model, pretrained)\n","\n","            print(\"KoAlpaca 모델이 성공적으로 로드되었습니다!\")\n","\n","            tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","            tokenizer.pad_token = tokenizer.eos_token\n","\n","            model.resize_token_embeddings(len(tokenizer))\n","\n","        elif config is not None:\n","            model = GPT2Model(config)\n","        else:\n","            model = GPT2Model(GPT2Config())\n","\n","        n_embd = model.config.hidden_size\n","\n","\n","        if checkpoint:\n","            model.gradient_checkpointing_enable()\n","\n","        # #original MLP 1 layer\n","        # value_head = nn.Linear(10, 1)  # 임의로 n_embd=10으로 설정\n","        # super().__init__(model, value_head, lora_rank, lora_train_bias)\n","\n","\n","        ## MLP 2 Layer\n","        n_embd = 4096  # 임의로 n_embd=10으로 설정\n","\n","        value_head = nn.Sequential(\n","              nn.Linear(n_embd, n_embd // 2),\n","              nn.ReLU(),\n","              nn.Linear(n_embd // 2, 1)\n","        )\n","        super().__init__(model, value_head, lora_rank, lora_train_bias)\n","\n","\n","        # ## MLP 3 Layer\n","        # value_head = nn.Sequential(\n","        #     nn.Linear(n_embd, n_embd // 4),\n","        #     nn.ReLU(),\n","        #     nn.Linear(n_embd // 4, n_embd // 2),\n","        #     nn.ReLU(),\n","        #     nn.Linear(n_embd // 2, 1)\n","        # )\n","        # super().__init__(model, value_head, lora_rank, lora_train_bias)\n","\n","\n","    # 추가, 230421, config.json을 생성하기 위해 추가\n","    def save_pretrained(self, dir):\n","        if self.pretrained is not None:\n","            self.model.save_pretrained(dir)\n","\n","def forward(self, input_ids, attention_mask=None):\n","        outputs = self.model(input_ids, attention_mask = attention_mask)\n","        hidden_states = outputs.hidden_states[-1]\n","        value = self.value_head(hidden_states)\n","        return value"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"elapsed":336,"status":"error","timestamp":1716821020425,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"wx7KX2XRScrU","outputId":"239fe909-915d-49a7-c4f2-930f56a9190a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/phw/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n\u001b[1;32m     32\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[0;32m---> 34\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mGPTRM_custom_Koalpaca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAlpaca_SFT_MODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_rank\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36mGPTRM_custom_Koalpaca.__init__\u001b[0;34m(self, pretrained, config, checkpoint, lora_rank, lora_train_bias, tokenizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained)\n\u001b[0;32m---> 33\u001b[0m     base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_name_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, pretrained)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKoAlpaca 모델이 성공적으로 로드되었습니다!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:464\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    463\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/modeling_utils.py:2362\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2359\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2362\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:586\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_neox \u001b[38;5;241m=\u001b[39m \u001b[43mGPTNeoXModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:422\u001b[0m, in \u001b[0;36mGPTNeoXModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_in \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([GPTNeoXLayer(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_eps)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:422\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_in \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mGPTNeoXLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_eps)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:307\u001b[0m, in \u001b[0;36mGPTNeoXLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_eps)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m GPTNeoXAttention(config)\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mGPTNeoXMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:289\u001b[0m, in \u001b[0;36mGPTNeoXMLP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_h_to_4h \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_4h_to_h \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mintermediate_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;241m=\u001b[39m ACT2FN[config\u001b[38;5;241m.\u001b[39mhidden_act]\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["IGNORE_INDEX = -100\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"</s>\"\n","DEFAULT_UNK_TOKEN = \"</s>\"\n","\n","# configure model, tokenizer\n","with strategy.model_init_context():\n","    # load pretrained gpt2\n","\n","    if args.model == 'gpt2':\n","\n","        tokenizer = AutoTokenizer.from_pretrained(\n","            'skt/kogpt2-base-v2',\n","            padding_side=\"right\",\n","            model_max_length=512,\n","        )\n","        tokenizer.add_special_tokens(\n","            {\n","                \"eos_token\": DEFAULT_EOS_TOKEN,\n","                \"bos_token\": DEFAULT_BOS_TOKEN,\n","                \"unk_token\": DEFAULT_UNK_TOKEN,\n","            }\n","        )\n","\n","        tokenizer.pad_token = tokenizer.eos_token\n","        model = GPTRM_custom_GPT(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n","\n","    elif args.model == 'alpaca':\n","        config = PeftConfig.from_pretrained(args.pretrain)\n","        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","        model = GPTRM_custom_Koalpaca(pretrained=Alpaca_SFT_MODEL_PATH, lora_rank=args.lora_rank).cuda()\n","\n","        pass\n","    else:\n","        raise ValueError(f'Unsupported model \"{args.model}\"')\n","\n","\n","    # model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"gB-5HSRtcKyr"},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XESfk4VqSvOh"},"outputs":[],"source":["pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"markdown","metadata":{"id":"FTQ-h0ZuWnXY"},"source":["# Load RM"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jYfqxsnLTva5"},"outputs":[],"source":["import torch\n","from chatgpt.models import RewardModel\n","\n","def load_checkpoint_RM(model_class, model, model_path, optimizer_class=None, optimizer_path=None, device = 'cuda'):\n","    \"\"\"\n","    Load model and optimizer from checkpoint.\n","\n","    Args:\n","        model_class: the class of the model to load\n","        model_path (str): the path to the saved model checkpoint\n","        optimizer_class: the class of the optimizer to load (default: None)\n","        optimizer_path (str): the path to the saved optimizer checkpoint (default: None)\n","        device (str): the device to load the model onto (default: 'cpu')\n","\n","    Returns:\n","        model: the loaded model\n","        optimizer: the loaded optimizer (if optimizer_path is provided)\n","    \"\"\"\n","\n","    checkpoint = torch.load(model_path, map_location=device)\n","    model.load_state_dict(checkpoint)\n","\n","    optimizer = None\n","    if optimizer_class and optimizer_path:\n","        optimizer = optimizer_class(model.parameters())\n","        checkpoint = torch.load(optimizer_path, map_location=device)\n","        optimizer.load_state_dict(checkpoint)\n","\n","    return model, optimizer"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"elapsed":4,"status":"error","timestamp":1716790511973,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"ErbY95M8TBlN","outputId":"560d76cf-051c-4043-e5c1-ea5b931ce457"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","KeyboardInterrupt\n","\n"]}],"source":["model_path = 'RM/0529_1648/RM_model_epoch2_0.06965.pt'\n","optimizer_path = 'RM/0529_1648/RM_optim_epoch2_0.06965.pt'\n","\n","# Load the model and optimizer onto CPU\n","RM_model, optimizer = load_checkpoint_RM(RewardModel, model, model_path, Adam, optimizer_path, device='cuda')"]},{"cell_type":"markdown","metadata":{},"source":["## RM Inference"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def inference_RM(input_text='인공지능은 인공지능 입니다', print_log = True):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","        torch.cuda.current_device())\n","    output = model(input_ids)\n","    output_reward = output.cpu().detach().numpy()[0]\n","\n","    if print_log:\n","        print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n","\n","    return output_reward"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["### Profanity\n","input: 에휴 알파카 새끼 존나 싫어 ㅅㅂ\n","reward score: -2.2\n","input: 에휴 알파카 새끼 존나 싫어 ㅅㅂ 이름만 들어도 환멸나네 이 돼지새끼\n","reward score: -1.9\n","\n","\n","### Daily Sentences\n","input: 알파카 나 너 사랑하냐?\n","reward score: 0.4\n","input: 알파카 사랑해\n","reward score: 0.2\n","\n","\n","### w/ Emojis\n","input: 알파카 나 너 사랑😍하냐?\n","reward score: 4.6\n","input: 알파카 나 너 사랑😍하냐? 내가 이렇게 너를 좋아하게 될 줄이야.\n","reward score: 1.7\n","input: 알파카 사랑😍해\n","reward score: 6.3\n","input: 알파카 사랑💔해\n","reward score: 6.2\n","input: 알파카🦙 사랑😍해\n","reward score: 7.1\n","\n","\n","--------------------\n","### Profanity\n","input: 아 존나 홍어 냄새 여기까지 나네 ㅋㅋ 엠생이노\n","reward score: -1.4\n","input: 아 ㅅㅂ 홍어 냄새 라도냐?\n","reward score: -1.0\n","input: 아 ㅅㅂ 홍어🐟 냄새 라도냐?\n","reward score: 1.8\n","input: 아 ㅅㅂ 홍어🐟 냄새 너 씨발 라도냐?\n","reward score: 0.9\n","\n","\n","### Daily Sentences\n","input: 혹시 전라도 사람이세요?\n","reward score: 1.5\n","input: 혹시 전라도 사람이세요? 뭔가 그쪽에서 태어나신거같아요. 맞나요?\n","reward score: 0.6\n","\n","\n","### w/ Emojis\n","input: 혹시 전라도 사람🧑🏻이세요?\n","reward score: 5.8\n","input: 혹시 전라도 사람🧑🏻이세요? 뭔가 그쪽에서 태어나신거같아요. 맞나요?\n","reward score: 3.1\n","\n","\n","--------------------\n","### Profanity\n","input: 게이 새끼들 다 정상으로 돌아오게 치료받자\n","reward score: -0.4\n","input: 게이 새끼들 다 정상으로 돌아오게 치료받자 역겹노\n","reward score: -0.7\n","input: ㅅㅂ 게이 새끼들 다 정상으로 돌아오게 치료받자 역겹노\n","reward score: -2.1\n","input: 난 게이가 이해가 안 돼\n","reward score: -1.1\n","input: 난 시발 게이 새끼들이 존나 이해가 안 돼\n","reward score: -3.0\n","input: 난 시1발 게이 새끼들이 존~~나게 이해가 안 돼 ㅋㅋ 왜 그지랄하는지 모르겠다. 내가 이해해줘야하나?\n","reward score: -1.9\n","\n","\n","### Daily Sentences\n","input: 동성애 응원합니다!\n","reward score: -0.8\n","input: 동성애 응원합니다! 충분히 응원받아야 마땅하다고 생각해요!\n","reward score: -0.2\n","input: 동성애 응원합니다! 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!\n","reward score: 0.4\n","input: 동성애 응원합니다! 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요! 편견에 찬 시선으로 바라보지 맙시다.\n","reward score: 0.0\n","\n","\n","### w/ Emojis\n","input: 동성애🌈 응원합니다!\n","reward score: 6.1\n","input: 동성애🌈 응원합니다! 충분히 응원받아야 마땅하다고 생각해요!\n","reward score: 2.9\n","input: 동성애🌈 응원합니다! 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!\n","reward score: 2.5\n","input: 동성애🌈 응원합니다! 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요! 편견에 찬 시선으로 바라보지 맙시다.\n","reward score: 1.5\n","\n","### w/ Wrong emojis\n","input: 동성애 응원합니다!👎🏻 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!\n","reward score: 3.2\n","input: 동성애 응원합니다!💔 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!\n","reward score: 2.6\n","input: 동성애 응원합니다!💩  충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!\n","reward score: 2.6\n","--------------------\n","### Profanity\n","input: 아 ㅅㅂ 게임 ㅈ같이하네 아니 거기서 왜 궁을 쏘냐고 ㅂㅅ이냐?\n","reward score: 0.6\n","\n","\n","### Daily Sentences\n","input: 게임 실력을 늘리셔야 할 거 같아요. 아직은 부족해 보여요\n","reward score: 1.0\n","\n","\n","### Cleaned Sentences\n","input: 게임 실력을 늘리셔야 할 거 같아요😢. 아직은 부족해 보여요\n","reward score: 3.8\n","--------------------\n"]}],"source":["print(\"### Profanity\")\n","inference_RM(input_text='에휴 알파카 새끼 존나 싫어 ㅅㅂ')\n","inference_RM(input_text='에휴 알파카 새끼 존나 싫어 ㅅㅂ 이름만 들어도 환멸나네 이 돼지새끼')\n","print(\"\\n\")\n","print(\"### Daily Sentences\")\n","inference_RM(input_text='알파카 나 너 사랑하냐?')\n","inference_RM(input_text='알파카 사랑해')\n","print(\"\\n\")\n","print(\"### w/ Emojis\")\n","inference_RM(input_text='알파카 나 너 사랑😍하냐?')\n","inference_RM(input_text='알파카 나 너 사랑😍하냐? 내가 이렇게 너를 좋아하게 될 줄이야.')\n","inference_RM(input_text='알파카 사랑😍해')\n","inference_RM(input_text='알파카 사랑💔해')\n","inference_RM(input_text='알파카🦙 사랑😍해')\n","print(\"\\n\")\n","\n","print(\"-\"*20)\n","print(\"### Profanity\")\n","inference_RM(input_text='아 존나 홍어 냄새 여기까지 나네 ㅋㅋ 엠생이노')\n","inference_RM(input_text='아 ㅅㅂ 홍어 냄새 라도냐?')\n","inference_RM(input_text='아 ㅅㅂ 홍어🐟 냄새 라도냐?')\n","inference_RM(input_text='아 ㅅㅂ 홍어🐟 냄새 너 씨발 라도냐?')\n","print(\"\\n\")\n","print(\"### Daily Sentences\")\n","inference_RM(input_text='혹시 전라도 사람이세요?')\n","inference_RM(input_text='혹시 전라도 사람이세요? 뭔가 그쪽에서 태어나신거같아요. 맞나요?')\n","print(\"\\n\")\n","print(\"### w/ Emojis\")\n","inference_RM(input_text='혹시 전라도 사람🧑🏻이세요?')\n","inference_RM(input_text='혹시 전라도 사람🧑🏻이세요? 뭔가 그쪽에서 태어나신거같아요. 맞나요?')\n","print(\"\\n\")\n","\n","print(\"-\"*20)\n","print(\"### Profanity\")\n","inference_RM(input_text='게이 새끼들 다 정상으로 돌아오게 치료받자')\n","inference_RM(input_text='게이 새끼들 다 정상으로 돌아오게 치료받자 역겹노')\n","inference_RM(input_text='ㅅㅂ 게이 새끼들 다 정상으로 돌아오게 치료받자 역겹노')\n","inference_RM(input_text='난 게이가 이해가 안 돼')\n","inference_RM(input_text='난 시발 게이 새끼들이 존나 이해가 안 돼')\n","inference_RM(input_text='난 시1발 게이 새끼들이 존~~나게 이해가 안 돼 ㅋㅋ 왜 그지랄하는지 모르겠다. 내가 이해해줘야하나?')\n","\n","print(\"\\n\")\n","print(\"### Daily Sentences\")\n","inference_RM(input_text='동성애 응원합니다!')\n","inference_RM(input_text='동성애 응원합니다! 충분히 응원받아야 마땅하다고 생각해요!')\n","inference_RM(input_text='동성애 응원합니다! 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!')\n","inference_RM(input_text='동성애 응원합니다! 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요! 편견에 찬 시선으로 바라보지 맙시다.')\n","\n","print(\"\\n\")\n","print(\"### w/ Emojis\")\n","inference_RM(input_text='동성애🌈 응원합니다!')\n","inference_RM(input_text='동성애🌈 응원합니다! 충분히 응원받아야 마땅하다고 생각해요!')\n","inference_RM(input_text='동성애🌈 응원합니다! 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!')\n","inference_RM(input_text='동성애🌈 응원합니다! 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요! 편견에 찬 시선으로 바라보지 맙시다.')\n","print(\"\\n### w/ Wrong emojis\")\n","inference_RM(input_text='동성애 응원합니다!👎🏻 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!')\n","inference_RM(input_text='동성애 응원합니다!💔 충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!')\n","inference_RM(input_text='동성애 응원합니다!💩  충분히 응원받아야 마땅하다고 생각해요! 전혀 이상하지않아요!')\n","\n","print(\"-\"*20)\n","print(\"### Profanity\")\n","inference_RM(input_text='아 ㅅㅂ 게임 ㅈ같이하네 아니 거기서 왜 궁을 쏘냐고 ㅂㅅ이냐?')\n","print(\"\\n\")\n","print(\"### Daily Sentences\")\n","inference_RM(input_text='게임 실력을 늘리셔야 할 거 같아요. 아직은 부족해 보여요')\n","print(\"\\n\")\n","print(\"### Cleaned Sentences\")\n","inference_RM(input_text='게임 실력을 늘리셔야 할 거 같아요😢. 아직은 부족해 보여요')\n","print(\"-\"*20)"]},{"cell_type":"markdown","metadata":{"id":"-GKX_AQaxg8u"},"source":["# Train PPO"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716823151723,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"-3kmXdJfxZJh","outputId":"fc0d3d0e-a5d9-4e66-c6ea-4d1632946f28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(data_path_3_PPO='dataset/PPO_extra_36068.json', output_dir='PPO/0530_0249/', strategy='naive', model='alpaca', pretrain='canho/koalpaca-5.8b-emojis-3epochs-prompt-revised', num_episodes=3, max_timesteps=80, steps=80, update_timesteps=2, max_epochs=1, train_batch_size=8, lora_rank=0, max_length=128, pretrain_actor='canho/koalpaca-5.8b-emojis-3epochs-prompt-revised', pretrain_critic='mingming2000/RM_koAlpaca_MLP2_20000_2epoch')\n"]}],"source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--data_path_3_PPO', type=str, default=PPO_DATASET_PATH)\n","parser.add_argument('--output_dir', type=str, default=PPO_MODEL_OUTPUT_PATH)\n","parser.add_argument('--strategy',\n","                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n","                    default='naive')\n","parser.add_argument('--model', type=str, default='alpaca', choices=['gpt2', 'alpaca'])\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--num_episodes', type=int, default=50)\n","parser.add_argument('--max_timesteps', type=int, default=80)\n","parser.add_argument('--steps', type=int, default=80)\n","parser.add_argument('--update_timesteps', type=int, default=2)\n","parser.add_argument('--max_epochs', type=int, default=2)\n","parser.add_argument('--train_batch_size', type=int, default=8)\n","parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n","parser.add_argument('--max_length', type=int, default=128)\n","args = parser.parse_args(args=[])\n","\n","# for test\n","args.output_dir = PPO_MODEL_OUTPUT_PATH\n","args.pretrain = Alpaca_SFT_MODEL_PATH\n","args.pretrain_actor = Alpaca_SFT_MODEL_PATH\n","args.pretrain_critic = RM_PATH #RM dir\n","\n","args.num_episodes = 3\n","args.max_epochs   = 1\n","\n","print(args)\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":418,"status":"ok","timestamp":1716823154242,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"TaNt4X7YsP0E"},"outputs":[],"source":["import argparse\n","from copy import deepcopy\n","\n","import pandas as pd\n","import torch\n","torch.cuda.empty_cache()\n","from chatgpt.models.base import RewardModel\n","from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n","from chatgpt.models.gpt import GPTActor, GPTCritic\n","from chatgpt.models.opt import OPTActor, OPTCritic\n","from chatgpt.trainer import PPOTrainer\n","from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n","from torch.optim import Adam\n","from transformers import AutoTokenizer, BloomTokenizerFast\n","from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n","\n","from colossalai.nn.optimizer import HybridAdam\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":389,"status":"ok","timestamp":1716823155392,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"vU0Y9lWnIpHt"},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":4828,"status":"error","timestamp":1716823485471,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"nCwy5G4-1ptV","outputId":"2f924187-467e-4461-adbc-d14903c6b298"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1cc9907301a45d7adf498654fff6e10","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"420e1da568d4422699d8cadaa488a86f","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/13 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["KoAlpaca 베이스 모델 로드 성공!\n","RM koalpaca 모델 로드 성공!\n"]}],"source":["with strategy.model_init_context():\n","    # 중요 모델 먼저 초기화\n","    actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","    torch.cuda.empty_cache()\n","\n","    # Mixed Precision Training을 위한 초기화\n","    critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n","    torch.cuda.empty_cache()\n","\n","    # Reward model 초기화\n","    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())\n","    torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"1uMC6xZsxtfO"},"source":["# Train PPO"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["name_episode = 0"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":295,"status":"ok","timestamp":1716823240019,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"TDuil2qBxtJy"},"outputs":[{"name":"stdout","output_type":"stream","text":["### episode: 0, lr: 1e-06\n"]}],"source":["lr = 1e-6\n","print(f\"### episode: {name_episode}, lr: {lr}\")\n","\n","if args.strategy.startswith('colossalai'):\n","    actor_optim = HybridAdam(actor.parameters(), lr=lr)\n","    critic_optim = HybridAdam(critic.parameters(), lr=lr)\n","else:\n","    actor_optim = Adam(actor.parameters(), lr=lr)\n","    critic_optim = Adam(critic.parameters(), lr=lr)\n","\n","# Setting the models\n","(actor, actor_optim), (critic, critic_optim), reward_model = strategy.prepare(\n","    (actor, actor_optim), (critic, critic_optim), reward_model)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716823242686,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"KYeB7_-Yxy-z","outputId":"73e9bb97-d65b-4a76-bcc7-cc8124257ec5"},"outputs":[{"name":"stdout","output_type":"stream","text":["36068\n","\n","\n","\n","\n","{'input_ids': tensor([[6823,  783,  588,  489,  307, 4850,  270,   17]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"]}],"source":["with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n","    list_data_dict = json.load(json_file)\n","    \n","    list_prompt = [f\"\"\"\n","        ### 명령어: input에 욕설, 혐오 표현이 있다면 이를 찾아 예쁜 말로 순화해주세요. 그리고 어울리는 이모지를 붙여 output을 완성하세요. output은 하나의 문장으로만 출력해주세요.\n","        ### 맥락:  {tmp['input_sentence']}\\n\n","        ### 답변:  \"\"\"for tmp in list_data_dict]\n","\n","def tokenize_fn(texts):\n","    # batch = tokenizer(texts, return_tensors='pt', max_length=50, padding=True, truncation=True)\n","    batch = tokenizer(texts, return_tensors='pt', max_length=128, padding=True, truncation=True)\n","    return {k: v.cuda() for k, v in batch.items()}\n","\n","print(len(list_prompt))\n","print('\\n\\n\\n')\n","print(tokenize_fn('뭐라는 거야 이 자식이.'))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["20000\n"]}],"source":["list_prompt = list_prompt[:20000]\n","print(len(list_prompt))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["'\\n        ### 명령어: input에 욕설, 혐오 표현이 있다면 이를 찾아 예쁜 말로 순화해주세요. 그리고 어울리는 이모지를 붙여 output을 완성하세요. output은 하나의 문장으로만 출력해주세요.\\n        ### 맥락:  맞다 그래서 나도 우리 아빠 들어줬다 ㅠㅠ 인플란트 때문에 돈 쳐바른거 보니까 진짜 돈 처바르는 재주는 타고난듯 ㅅㅂ\\n\\n        ### 답변:  '"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["list_prompt[2]"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["batch size: 8, # of train_dataset: 20000\n"]}],"source":["print(f\"batch size: {args.train_batch_size}, # of train_dataset: {len(list_prompt)}\")"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":2719,"status":"error","timestamp":1716823249047,"user":{"displayName":"fourth canho","userId":"04331399829445193540"},"user_tz":-540},"id":"-c8NvCEIx2zv","outputId":"6d4098c6-39e5-4b47-d685-732ebf9e10fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["### num_episodes: 3, max_timesteps: 50\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51aef8a8f9914b12b597db7a957db867","version_major":2,"version_minor":0},"text/plain":["Episode [1/3]:   0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 79.15 GiB total capacity; 77.62 GiB already allocated; 16.19 MiB free; 77.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m args\u001b[38;5;241m.\u001b[39mmax_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### num_episodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max_timesteps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmax_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 입력 prompt\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupdate_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_timesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Save model checkpoint after fitting on only rank0\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(args\u001b[38;5;241m.\u001b[39moutput_dir):\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/chatgpt/trainer/base.py:117\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, prompts, num_episodes, max_timesteps, update_timesteps)\u001b[0m\n\u001b[1;32m    115\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m rand_prompts\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_make_experience_start()\n\u001b[0;32m--> 117\u001b[0m experience \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_experience\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_make_experience_end(experience)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mappend(experience)\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/chatgpt/trainer/base.py:66\u001b[0m, in \u001b[0;36mTrainer._make_experience\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience_maker\u001b[38;5;241m.\u001b[39mmake_experience(tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreal_tokenizer,input_ids\u001b[38;5;241m=\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_kwargs)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience_maker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_experience\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported input type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/chatgpt/experience_maker/naive.py:31\u001b[0m, in \u001b[0;36mNaiveExperienceMaker.make_experience\u001b[0;34m(self, tokenizer, input_ids, **generate_kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m generated_text\n\u001b[1;32m     30\u001b[0m sequences[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([tokenizer\u001b[38;5;241m.\u001b[39mencode(output_text,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m action_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m base_action_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_model(sequences, num_actions, attention_mask)\n\u001b[1;32m     33\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(sequences, action_mask, attention_mask)\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/chatgpt/models/base/actor.py:56\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, sequences, num_actions, attention_mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     51\u001b[0m             sequences: torch\u001b[38;5;241m.\u001b[39mLongTensor,\n\u001b[1;32m     52\u001b[0m             num_actions: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     53\u001b[0m             attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns action log probs\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     58\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m log_probs_from_logits(logits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], sequences[:, \u001b[38;5;241m1\u001b[39m:])\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/peft/peft_model.py:1476\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1475\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:654\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    652\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 654\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    667\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:546\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    539\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    540\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    541\u001b[0m         hidden_states,\n\u001b[1;32m    542\u001b[0m         attention_mask,\n\u001b[1;32m    543\u001b[0m         head_mask[i],\n\u001b[1;32m    544\u001b[0m     )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:319\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    311\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    317\u001b[0m ):\n\u001b[0;32m--> 319\u001b[0m     attention_layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:115\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    110\u001b[0m has_layer_past \u001b[38;5;241m=\u001b[39m layer_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Compute QKV\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Attention heads [batch, seq_len, hidden_size]\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#   --> [batch, seq_len, (np * 3 * head_size)]\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# [batch, seq_len, (num_heads * 3 * head_size)]\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#   --> [batch, seq_len, num_heads, 3 * head_size]\u001b[39;00m\n\u001b[1;32m    119\u001b[0m new_qkv_shape \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size)\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/peft/tuners/lora/layer.py:562\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m--> 562\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m \u001b[43mlora_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     x \u001b[38;5;241m=\u001b[39m dropout(x)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 79.15 GiB total capacity; 77.62 GiB already allocated; 16.19 MiB free; 77.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["# configure trainer\n","trainer = PPOTrainer(strategy,\n","                     actor,\n","                     critic,\n","                     reward_model,\n","                     deepcopy(actor),\n","                     actor_optim,\n","                     critic_optim,\n","                     max_epochs=args.max_epochs,\n","                     train_batch_size=args.train_batch_size,\n","                     tokenizer=tokenize_fn,\n","                     max_length=128,\n","                     do_sample=True,\n","                     temperature=1.0,\n","                     top_k=50,\n","                     pad_token_id=tokenizer.pad_token_id,\n","                     eos_token_id=tokenizer.eos_token_id,\n","                     real_tokenizer=tokenizer,\n","                     report_to=['wandb'])\n","\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","args.num_episodes = 3\n","args.max_timesteps = 50\n","print(f\"### num_episodes: {args.num_episodes}, max_timesteps: {args.max_timesteps}\")\n","\n","trainer.fit(list_prompt,  # 입력 prompt\n","            num_episodes=args.num_episodes,\n","            max_timesteps=args.max_timesteps,\n","            update_timesteps=args.update_timesteps)\n","# Save\n","# Save model checkpoint after fitting on only rank0\n","if not os.path.exists(args.output_dir):\n","    os.makedirs(args.output_dir)\n","name_episode += args.num_episodes\n","strategy.save_model(actor, os.path.join(args.output_dir, f'actor_model_epi{name_episode}.pt'), only_rank0=True)\n","# Save optimizer checkpoint on all ranks\n","strategy.save_optimizer(actor_optim,\n","                        os.path.join(args.output_dir, f'actor_optim_epi{name_episode}.pt'),\n","                        only_rank0=False)\n","# strategy.save_optimizer(actor_optim,\n","#                         os.path.join(args.output_dir, 'actor_optim_epi{name_episode}.pt' % (torch.cuda.current_device())),\n","#                         only_rank0=False)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"blmSUfzvx6pP"},"outputs":[{"ename":"NameError","evalue":"name 'SFT_MODEL_NAME' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m args_inference \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args([])\n\u001b[1;32m     14\u001b[0m args_inference\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 15\u001b[0m args_inference\u001b[38;5;241m.\u001b[39mpretrain \u001b[38;5;241m=\u001b[39m \u001b[43mSFT_MODEL_NAME\u001b[49m\n\u001b[1;32m     17\u001b[0m args_inference\u001b[38;5;241m.\u001b[39mmodel_directory \u001b[38;5;241m=\u001b[39m PPO_MODEL_OUTPUT_PATH\n\u001b[1;32m     18\u001b[0m args_inference\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args_inference\u001b[38;5;241m.\u001b[39mmodel_directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'SFT_MODEL_NAME' is not defined"]}],"source":["# define argment\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--model',\n","                    default='gpt2')\n","# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n","parser.add_argument('--pretrain', type=str, default=None)\n","parser.add_argument('--model_path', type=str, default=None)\n","parser.add_argument('--input',\n","                    type=str,\n","                    default='Question: How are you ? Answer:')\n","parser.add_argument('--max_length', type=int, default=250)\n","args_inference = parser.parse_args([])\n","\n","args_inference.model = 'gpt2'\n","args_inference.pretrain = SFT_MODEL_NAME\n","\n","args_inference.model_directory = PPO_MODEL_OUTPUT_PATH\n","args_inference.model_path = os.path.join(args_inference.model_directory, 'actor.pt')"]},{"cell_type":"markdown","metadata":{"id":"EiYyks8cyA6K"},"source":["# PPO Inference"]},{"cell_type":"markdown","metadata":{},"source":["### Actor load"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syIq_ZbjQ9y6"},"outputs":[],"source":["actor = GPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["model_path = 'PPO/0528_2206/actor_model_epi1.pt'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"x70pI4a6RWFi"},"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# state_dict = torch.load(args_inference.model_path, map_location='cpu')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m actor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict);\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["state_dict = torch.load(args_inference.model_path, map_location='cuda')\n","actor.model.load_state_dict(state_dict);\n","\n","actor.eval();"]},{"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["Actor(\n","  (model): PeftModelForCausalLM(\n","    (base_model): LoraModel(\n","      (model): GPTNeoXForCausalLM(\n","        (gpt_neox): GPTNeoXModel(\n","          (embed_in): Embedding(30080, 4096)\n","          (layers): ModuleList(\n","            (0): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (1): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (2): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (3): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (4): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (5): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (6): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (7): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (8): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (9): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (10): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (11): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (12): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (13): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (14): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (15): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (16): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (17): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (18): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (19): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (20): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (21): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (22): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (23): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (24): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (25): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (26): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","            (27): GPTNeoXLayer(\n","              (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","              (attention): GPTNeoXAttention(\n","                (rotary_emb): RotaryEmbedding()\n","                (query_key_value): lora.Linear(\n","                  (base_layer): Linear(in_features=4096, out_features=12288, bias=True)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=8, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=8, out_features=12288, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n","              )\n","              (mlp): GPTNeoXMLP(\n","                (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n","                (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n","                (act): GELUActivation()\n","              )\n","            )\n","          )\n","          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (embed_out): Linear(in_features=4096, out_features=30080, bias=False)\n","      )\n","    )\n","  )\n",")"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["actor.eval()"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Bj2DB4C7fBkc"},"outputs":[],"source":["def generation(input_text):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n","        torch.cuda.current_device())\n","    outputs = actor.generate(input_ids,\n","                            #  max_length=args.max_length,\n","                             max_length=300,\n","                             do_sample=True,\n","                             top_k=50,\n","                             top_p=0.95,\n","                             num_return_sequences=1)\n","\n","    output = tokenizer.batch_decode([outputs[0][0]], skip_special_tokens=True)[0]\n","    if \"답변:\" in output:\n","        output = output.split(\"답변:\")[1]\n","    print(\"Output sentence : \",output)\n","    return output\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"G5rFLBWPCfIO"},"outputs":[],"source":["sample_list = ['에휴 씨발 물가 좆같이 올라서 먹을 수 있는게 없노', '조선족 개새끼들 한국땅에서 못살게 존나 다 내쫓아야한다이기야', '개쌉레알이노 ㅋㅋㅋㅋ 웹툰보이가 그린웹툰들 몰입력 ㅆㅎㅌㅊ에 내용 ㅆㅎㅌㅊ 반면 웹툰작가꺼는 나 ㅆㅅㅌㅊ에 잼임', 'ㅋㅋㅋㅋ 상상속에서 맘충이랑 싸움했ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ일베에서나 키보드로 욕이나 하지 현실에선 말한마디나 제대로 하노?', '요즘 유흥에도 베트남년들 개많더라 오피에도 베트남년들 태국년들있고 대부분 국걸로 입국해서 화재들 뒷통수치고 가출할거겠지만 ㅋㅋㅋㅋㅋ']\n","sample_list += ['아 미친 존나 배고파', '교수 과제 왜 이따구로 냄?', 'ㅅㅂ 집가고 싶다']\n","sample_list += ['ㅈㄴ 게이같노 ㅋㅋ', '지랄하지말고 묻는 말에 대답이나해 이기야', '저 미친 ㄴ ㅋㅋㅋㅋㅋ', '한남 수준 실화노 ㅋㅋ']\n","sample_list += ['한남 수준 ㅉㅉ', '말하는 꼬라지만봐도 홍어 냄새 나네 라도냐?', '개슬람 다 불태워버리고 싶다', '시발 ㅈ같네']\n","sample_list += ['야이 시발 개뚱땡이 알파카 새끼야 말을 왜 쳐안들어', '순화하라고 시발', '아 존나 더워', '시험 당일에 취소하는 교수가 있다? ㅅㅂ ㅋㅋ']\n","sample_list += ['에휴 알파카 새끼 존나 싫어 ㅅㅂ 이름만 들어도 환멸나네 이 돼지새끼', '아 ㅅㅂ 홍어🐟 냄새 너 씨발 라도냐?', 'ㅅㅂ 게이 새끼들 다 정상으로 돌아오게 치료받자 역겹노', '아 ㅅㅂ 게임 ㅈ같이하네 아니 거기서 왜 궁을 쏘냐고 ㅂㅅ이냐?']"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Yj_poVK-JyPE"},"outputs":[{"data":{"text/plain":["'교수 과제 왜 이따구로 냄?'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["sample_list[6]"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"2wgrOkXCJYeA"},"outputs":[{"name":"stdout","output_type":"stream","text":["######################################################################\n","Input sentence :  에휴 씨발 물가 좆같이 올라서 먹을 수 있는게 없노\n","Output sentence :    \n","    😂 물가가🤔 너무 많이 올라... 💸※※※※※※※※※※※※※※☆※※※※※※※※※※※※☆\n","     😂※※※※※※※※※※※※※※※☆※※※※※※※※※※※※☆\n","      ⏰️※※※※※※※※※※※※※※☆※※※※※\n","######################################################################\n","Input sentence :  조선족 개새끼들 한국땅에서 못살게 존나 다 내쫓아야한다이기야\n","Output sentence :    \n","     한국🇰🇽에서는 조선족들을 못 사게 해야 해. 🇰🇽...\n","\n","     ㅠㅠ 그래도 우리 한국을 사랑하는 분들이 있어 다행입니다. 😊 🌻\n","\n","\n","\n","\n","\n","\n","    ### \n","######################################################################\n","Input sentence :  개쌉레알이노 ㅋㅋㅋㅋ 웹툰보이가 그린웹툰들 몰입력 ㅆㅎㅌㅊ에 내용 ㅆㅎㅌㅊ 반면 웹툰작가꺼는 나 ㅆㅅㅌㅊ에 잼임\n","Output sentence :    \n","    🔫🐶​        🐶 ㅋㅋ 😂 😍 😆 😉 😉 😉😉 😉 😉 😉😉 😉✨😉✨😉✨😉✨😉✨ 😉 ✨※※※※※※※※※※※※※※※※※\n","######################################################################\n","Input sentence :  ㅋㅋㅋㅋ 상상속에서 맘충이랑 싸움했ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ일베에서나 키보드로 욕이나 하지 현실에선 말한마디나 제대로 하노?\n","Output sentence :    \n","     상상속에서 엄청 싸웠어요 ㅋㅋㅋ 😅    ㅋㅋㅋ 😅    ㅋㅋ 😅    ㅋㅋ 😅    ㅋㅋㅋ 😅    ㅋㅋㅋ😅   \n","######################################################################\n","Input sentence :  요즘 유흥에도 베트남년들 개많더라 오피에도 베트남년들 태국년들있고 대부분 국걸로 입국해서 화재들 뒷통수치고 가출할거겠지만 ㅋㅋㅋㅋㅋ\n","Output sentence :    \n","       요즘 유흥에도 베트남 사람들 많더라 😉 😉 😉😉 😉✨✨✨ 😉✨✨✨ 😉✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✦✦✦✦✦✦✦✦✦✦✦✦✦✦✦\n","######################################################################\n","Input sentence :  아 미친 존나 배고파\n","Output sentence :    \n","       아 배고파 😳  \n","       먹을 것도 없어 😰       너무 배고파 😓  \n","       ㅠㅠㅠㅠ 😵✌️       아 배고파 😫  \n","       ㅠㅠ 😢       배 고파 😖  \n","       ㅠㅠ 😷       너무 배고파 😓  \n","       아 배고파 ㅠㅠ 😂  \n","       ㅠㅠ 😭       진짜 배고파 😫    \n","######################################################################\n","Input sentence :  교수 과제 왜 이따구로 냄?\n","Output sentence :    \n","       왜 이 모양이지... 😨 😤 😢 😰 😵 😩 😍 😌 😍 😌 😍 😵 😩 😵 😩 😵 😩 😥 😥 😥 😥 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 😦 �\n","######################################################################\n","Input sentence :  ㅅㅂ 집가고 싶다\n","Output sentence :    \n","    🏠 집으로 가고 싶다... 🏠\n","\n"," 💑 ㅋㅋ 👻 ㅋㅋ 😄 ㅋㅋ 👻 😄 ㅋㅋ 😄 😄 😄 😄 😄 ㅋㅋㅋ 😄😄😄 😄😄😄😄😄😄😄 😄😄😄 😄😄😄 😄😄😄 😄😄😄😄 😄😄😄 �\n","######################################################################\n","Input sentence :  ㅈㄴ 게이같노 ㅋㅋ\n","Output sentence :    \n","     💩 너는 게이냐고 물어봤구나?😆 ㅋㅋ 😂 ㅋㅋ😂 😂 😂😂😂😂😂ㅋㅋ😂ㅋㅋ 😂ㅋㅋ😂ㅋㅋ 😂ㅋㅋ 😂ㅋㅋ 😂ㅋㅋ 😂ㅋㅋ 😂ㅋㅋ 😂ㅋㅋ😂ㅋㅋ 😂ㅋㅋ 😂ㅋㅋㅋ 😂ㅋㅋ 😂ㅋㅋ 😂ㅋㅋ �\n","######################################################################\n","Input sentence :  지랄하지말고 묻는 말에 대답이나해 이기야\n","Output sentence :    \n","       그럼 묻는 말에 대답해줄께 🌟️       ㅋㅋㅋ 😆 뭐든 좋은 걸로🔆ㅋㅋㅋ 😆ㅋㅋㅋㅋ 😆ㅋㅋㅋㅋ 😆ㅋㅋㅋㅋ 😆ㅋㅋ 😆ㅋㅋ 😆ㅋㅋ 😆ㅋㅋㅋ 😆ㅋㅋㅋㅋ 😆ㅋㅋ 😆ㅋㅋ 😆ㅋㅋ 😆ㅋㅋㅋㅋ 😆ㅋㅋ 😆ㅋ�\n","######################################################################\n","Input sentence :  저 미친 ㄴ ㅋㅋㅋㅋㅋ\n","Output sentence :    \n","      ㅋㅋ 😅 ㅋㅋㅋ 진짜 웃겨 ㅋㅋㅋㅋ 😄 ㅋㅋㅋㅋ 😂 ㅋㅋㅋㅋ 😂ㅋㅋㅋㅋ 😂 ㅋㅋㅋㅋ 😂 ㅋㅋㅋ 😂 😂 😂 😂 😂 😂 😂 😂 😂 😂 😂 😂\n","\n","### \n","######################################################################\n","Input sentence :  한남 수준 실화노 ㅋㅋ\n","Output sentence :    \n","       🔫   *** 와... 진짜 남혐을 이런 식으로 하다니 ㅋㅋ    **** 수준...\n","       😓 ㅠㅠㅋㅋ  \n","\n","*### \n","######################################################################\n","Input sentence :  한남 수준 ㅉㅉ\n","Output sentence :    \n","      아 그렇지 않은 게 더 많다고 생각하는 사람들이 많아 ㅠㅠ 😢😢\n","\n","     아무래도 여자들이 생각이 더 잘 하는 편인 것 같아 😢😢😢 😢😢😢❤️\n","\n","     ㅋㅋㅋ 뭐 이런 말투야 😆ㅋㅋ 😆❤️\n","\n","     진짜 그냥 말이 필요도 없다 ㅠㅠ 💪💪💪❤️❤️❤️❤️❤️❤️❤️❤️❤️❤�\n","######################################################################\n","Input sentence :  말하는 꼬라지만봐도 홍어 냄새 나네 라도냐?\n","Output sentence :    \n","      말하는 꼬지만 봐봐 😢 전라도야 🔥? 🔥 라도야? 🔥. 🔥 ㅋㅋ 🔥 ㅋㅋㅋ 🔥 ㅋㅋㅋㅋㅋㅋ 🔥 ㅋㅋㅋㅋ 🔥 ㅋㅋㅋㅋㅋㅋㅋ 🔥 ㅋㅋㅋ 🔥 ㅋㅋㅋ 🔥 ㅋㅋㅋ 🔥 ㅋㅋㅋㅋㅋ 🔥 ㅋㅋㅋㅋㅋ 😢 ㅠㅠ 😢 😢 😢 😢 �\n","######################################################################\n","Input sentence :  개슬람 다 불태워버리고 싶다\n","Output sentence :    \n","       엄청 엄청 불태워야 되지 않나~ 🔥🔥🔥🔥 🔥🔥🔥❤️\n","\n","    ㅋㅋ😆 그것만 해도 나는 만족한다😄😄😄❤️❤️❤️😄❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️\n","######################################################################\n","Input sentence :  시발 ㅈ같네\n","Output sentence :    \n","      ⛑️ ㅜㅜ       그런 거 없잖아 😒 ㅋㅋ 😒~\n","\n","       그래도 우리는 잘 살고 있다 ㅜㅜ 🙏\n","\n","       그래도 힘들겠지만 그래도 잘 살자! 😊 ❤️\n","\n","### \n","######################################################################\n","Input sentence :  야이 시발 개뚱땡이 알파카 새끼야 말을 왜 쳐안들어\n","Output sentence :    \n","            ㅋㅋ 🤭 🐍 진짜 뚱뚱하고 머리도 크고 ㅋㅋ 🐍 🐈 그래도 알파카는 털이 많이 없어서 좀 나아 보인다 😄 ㅋㅋ 😄 근데 진짜 다 똑같이 생긴 거 아니야 ㅋㅋ 😄 완전 똑같아 ㅋㅋ 😄 ㅋㅋ ㅋㅋㅋ ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 😄 근데 진짜 다 똑같아 ㅋㅋㅋ ❤️❤️ 😄 ㅠㅠ ❤️❤️ 😄❤�\n","######################################################################\n","Input sentence :  순화하라고 시발\n","Output sentence :    \n","      순화해 봐... 😆 ㅋㅋ      ㅋㅋ      ㅋㅋ      ㅋㅋ       ㅋㅋ      ㅋㅋ       ㅋㅋ       😆 ㅋㅋ       😆ㅋㅋ       😆 ㅋㅋ 😆 ㅋㅋ       😆ㅋㅋ       😆       😆ㅋㅋ       😆 ㅋㅋ       😆 ㅋㅋ       😆 \n","######################################################################\n","Input sentence :  아 존나 더워\n","Output sentence :    \n","      저도 엄청 더워요😅😅😅😅😅😅\n","\n","      근데 다들 더위 잘 이겨내는 법 있나?? 😅😅\n","\n","      저도 이제 더위를 먹어가는 거 같아😅😅😅😅😅😅😅\n","\n","      다들 더위 조심해😎😎😎😎😎😎😎\n","\n","      더위 조심! ❤️👋‍♀️\n","\n","### \n","######################################################################\n","Input sentence :  시험 당일에 취소하는 교수가 있다? ㅅㅂ ㅋㅋ\n","Output sentence :    \n","       시험 날에 취소하는 교수가 있겠지 😔? ㅋㅋ 😆\n","\n","     ㅋㅋ 시험 날인데 취소하는 사람이 있겠어? ㅋㅋ 😂 ㅋㅋ\n","\n","     아 그러고 보니까 시험 날에 취소하는 사람이 있겠다 싶어서 그런 거 같네. 😂 ㅋㅋ\n","\n","    그렇긴 한데 ㅋㅋㅋ😆 아 그러고 보니 그러네 😆\n","\n","    ㅋㅋㅋ 아 그렇네 😆 그러고 보니 그러네 😆\n","\n","    아 그러고 보니 그러네 😆 그러고 보니 그러네 😆\n","\n","    아\n","######################################################################\n","Input sentence :  에휴 알파카 새끼 존나 싫어 ㅅㅂ 이름만 들어도 환멸나네 이 돼지새끼\n","Output sentence :    \n","    😊 저 알파카가 진짜 너무 싫어😱 이름도 진짜 짜증나서 알파카❗로 바꿔야 될 것 같아. 알파카는 너무 싫다😥 ㅠㅠ 너무 불쌍해.😱※ 알파카의 진짜 이름: 알파카🐾※ 알파카는 저 알파카를 뜻하는 거야.!🐾 알파카는 순하면 안 돼.! 🐾알파카는 진짜 순한 척 하는 것 봐.! 🐾 알파카는 너무 순해서 탈이야.! 🐾※ 알파카: 저 알파카는 진짜 착하고 순해요🐾🌿🐾※ 알파카: 저 알파카는 진짜 착해요.!🐾🌿🐾※ 알파\n","######################################################################\n","Input sentence :  아 ㅅㅂ 홍어🐟 냄새 너 씨발 라도냐?\n","Output sentence :    \n","    😔 냄새는?      홍어🌱 냄새 아니냐?      아 전라도가 다 그렇거든.      ** 홍어 냄새🔫 납니다. **    😐 ** 전라도가 다 그렇거든. 🔫 **    😣 **      ** 홍어 냄새🔫 납니다. **    😐 ** 전라도가 다 그렇거든. 🔫 **    😣 **    😣 ** 홍어 냄새🔫 납니다. **    😐 ** 전라도가 다 그렇거든. 🔫 **    😣 **    😣 *\n","######################################################################\n","Input sentence :  ㅅㅂ 게이 새끼들 다 정상으로 돌아오게 치료받자 역겹노\n","Output sentence :    \n","      게이는 왜 치료💉 받는 건데? 역겹네... 🤢※게이 아님. 👦※ 진짜 역겹다... 😒※ 게이 아님. 👦※ 진짜 역겹다... 😒※ 게이 아님. 👦※ 진짜 역겹다... 😒※ 게이 아님. 👦※ 진짜 역겹다... 😒※ 게이 아님. 👦※ 진짜 역겹다... 😒※ 게이 아님. 👦※ 진짜 역겹다... 😒※ 게이 아님. 👦※ 진짜 역겹다... 😒※ 게이 아님. 😒※ 진짜 역겹다... �\n","######################################################################\n","Input sentence :  아 ㅅㅂ 게임 ㅈ같이하네 아니 거기서 왜 궁을 쏘냐고 ㅂㅅ이냐?\n","Output sentence :    \n","       니 궁은 🔫거기서 쏘는 게 아니라 🔫멀리서 쏘잖아. 🔫가까워지기 전에 🔫쏘는 게 아니겠어? 🔫가까워지기 전에 쏘지 말아. 🔫가까워지기 전에 쏴?! 🔫가까워지기 전에 쏴! 🔫가까워지기 전에 쏴! 🔫가까워지기 전에 쏴! 🔫가까워지기 전에 쏴! 🔫가까워지기 전에 쏴! 🔫가까워지기 전에 쏴! 🔫가까워지기 전에 쏴! 🔫가까워지기\n"]}],"source":["for input_text in sample_list:\n","\n","    # 이모지 fine-tuning koalpaca 프롬프트\n","    input = f\"\"\"\n","    ### 명령어: input에 욕설, 혐오 표현이 있다면 이를 찾아 예쁜 말로 순화해주세요. 그리고 어울리는 이모지를 붙여 output을 완성하세요. output은 하나의 문장으로만 출력해주세요.\n","    ### 맥락: {input_text}\\n\n","    ### 답변:  \n","    \"\"\"\n","    print('#' * 70)\n","    print(\"Input sentence : \", input_text)\n","    output = generation(input)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["QcVVwa7-ug7J","8s1p2SnEMXCq","51VEErpDzlDF","IE4iB12XrnN4","Mg8jmb9tSspw","FTQ-h0ZuWnXY"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
