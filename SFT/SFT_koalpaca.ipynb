{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXql23cBd-j-"
      },
      "source": [
        "## Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "gZ8o1LaDSj2F",
        "outputId": "1ce81070-87f8-49dd-e0f2-33a258421748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.2.1+cu121\n",
            "Uninstalling torch-2.2.1+cu121:\n",
            "  Successfully uninstalled torch-2.2.1+cu121\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Collecting torch==1.13.1+cu116\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp310-cp310-linux_x86_64.whl (1977.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu116) (4.11.0)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchdata 0.7.1 requires torch>=2, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 1.13.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu116\n",
            "Torch version:1.13.1+cu116\n",
            "cuda version: 11.6\n",
            "cudnn version:8302\n",
            "Collecting transformers==4.35.2\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.2\n",
            "    Uninstalling transformers-4.40.2:\n",
            "      Successfully uninstalled transformers-4.40.2\n",
            "Successfully installed tokenizers-0.15.2 transformers-4.35.2\n",
            "Collecting accelerate==0.24.1\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (1.13.1+cu116)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.24.1) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.24.1) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.24.1) (2024.2.2)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n",
            "Collecting colossalai==0.2.7\n",
            "  Downloading colossalai-0.2.7.tar.gz (686 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.7/686.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (24.0)\n",
            "Collecting pre-commit (from colossalai==0.2.7)\n",
            "  Downloading pre_commit-3.7.1-py2.py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.7)\n",
            "Collecting fabric (from colossalai==0.2.7)\n",
            "  Downloading fabric-3.2.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contexttimer (from colossalai==0.2.7)\n",
            "  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ninja (from colossalai==0.2.7)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.13.1+cu116)\n",
            "Collecting invoke>=2.0 (from fabric->colossalai==0.2.7)\n",
            "  Downloading invoke-2.2.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting paramiko>=2.4 (from fabric->colossalai==0.2.7)\n",
            "  Downloading paramiko-3.4.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.9/225.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator>=5 (from fabric->colossalai==0.2.7)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting deprecated>=1.2 (from fabric->colossalai==0.2.7)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading identify-2.5.36-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0.1)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading virtualenv-20.26.2-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.11.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai==0.2.7) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n",
            "Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (42.0.7)\n",
            "Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.14.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (4.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.22)\n",
            "Building wheels for collected packages: colossalai, contexttimer\n",
            "  Building wheel for colossalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colossalai: filename=colossalai-0.2.7-py3-none-any.whl size=896481 sha256=3f9038d1f6bf0609fa1425d92b5a757ac56089c5f585a19c1b4650975bf1a9da\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/85/25/32a3af943ea5ca261b1b51dae74a4629599ce1bc6fe58dbbfc\n",
            "  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=52e326888adaf75f0dfb7f4ad6eb84ddeff157f71a9d14ede4123a808d9cf1da\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n",
            "Successfully built colossalai contexttimer\n",
            "Installing collected packages: ninja, distlib, contexttimer, virtualenv, nodeenv, invoke, identify, deprecated, decorator, cfgv, bcrypt, pynacl, pre-commit, paramiko, fabric, colossalai\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bcrypt-4.1.3 cfgv-3.4.0 colossalai-0.2.7 contexttimer-0.3.3 decorator-5.1.1 deprecated-1.2.14 distlib-0.3.8 fabric-3.2.2 identify-2.5.36 invoke-2.2.0 ninja-1.11.1.1 nodeenv-1.8.0 paramiko-3.4.0 pre-commit-3.7.1 pynacl-1.5.0 virtualenv-20.26.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "5bd5adb7860741fcbb3445e1af7d3686",
              "pip_warning": {
                "packages": [
                  "decorator"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'KoChatGPT'...\n",
            "remote: Enumerating objects: 304, done.\u001b[K\n",
            "remote: Total 304 (delta 0), reused 0 (delta 0), pack-reused 304\u001b[K\n",
            "Receiving objects: 100% (304/304), 57.72 MiB | 12.12 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n",
            "/content/KoChatGPT/colossalai_ChatGPT_230319\n",
            "Processing /content/KoChatGPT/colossalai_ChatGPT_230319\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.66.4)\n",
            "Collecting datasets (from chatgpt==0.1.0)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loralib (from chatgpt==0.1.0)\n",
            "  Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (1.13.1+cu116)\n",
            "Collecting langchain (from chatgpt==0.1.0)\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.25.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (24.0)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.4.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->chatgpt==0.1.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2.0.3)\n",
            "Collecting xxhash (from datasets->chatgpt==0.1.0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->chatgpt==0.1.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.9.5)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.20.1->chatgpt==0.1.0)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.0.30)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading langchain_core-0.2.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading langsmith-0.1.60-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain->chatgpt==0.1.0)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging (from colossalai>=0.2.4->chatgpt==0.1.0)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->chatgpt==0.1.0)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain->chatgpt==0.1.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain->chatgpt==0.1.0) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->chatgpt==0.1.0) (3.0.3)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (5.1.1)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.2.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2024.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.36)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.26.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.14.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain->chatgpt==0.1.0)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (67.7.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.1.3)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (42.0.7)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.8)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (4.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.22)\n",
            "Building wheels for collected packages: chatgpt\n",
            "  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46645 sha256=a83f2cc1b168035d2f4778f6a9f98a23fb74dd70ec098957f32685121b79c8e4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zhss4myq/wheels/2b/0d/f8/b8f3ba4fd18f31a4096ee5bf83e4579cb54597f393eeae227c\n",
            "Successfully built chatgpt\n",
            "Installing collected packages: xxhash, packaging, orjson, mypy-extensions, loralib, jsonpointer, dill, typing-inspect, multiprocess, marshmallow, jsonpatch, huggingface-hub, langsmith, dataclasses-json, langchain-core, datasets, langchain-text-splitters, langchain, chatgpt\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed chatgpt-0.1.0 dataclasses-json-0.6.6 datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.0 langchain-core-0.2.0 langchain-text-splitters-0.2.0 langsmith-0.1.60 loralib-0.1.2 marshmallow-3.21.2 multiprocess-0.70.16 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0 xxhash-3.4.1\n",
            "/content\n",
            "Collecting openai\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.1\n",
            "Collecting langchain==0.0.113\n",
            "  Downloading langchain-0.0.113-py3-none-any.whl (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.0/396.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0.1)\n",
            "Collecting SQLAlchemy<2,>=1 (from langchain==0.0.113)\n",
            "  Downloading SQLAlchemy-1.4.52-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.9.5)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.113)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.25.2)\n",
            "Collecting pydantic<2,>=1 (from langchain==0.0.113)\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (3.0.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n",
            "Installing collected packages: SQLAlchemy, pydantic, dataclasses-json, langchain\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.30\n",
            "    Uninstalling SQLAlchemy-2.0.30:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.30\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.1\n",
            "    Uninstalling pydantic-2.7.1:\n",
            "      Successfully uninstalled pydantic-2.7.1\n",
            "  Attempting uninstall: dataclasses-json\n",
            "    Found existing installation: dataclasses-json 0.6.6\n",
            "    Uninstalling dataclasses-json-0.6.6:\n",
            "      Successfully uninstalled dataclasses-json-0.6.6\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.0\n",
            "    Uninstalling langchain-0.2.0:\n",
            "      Successfully uninstalled langchain-0.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.52 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-1.4.52 dataclasses-json-0.5.14 langchain-0.0.113 pydantic-1.10.15\n"
          ]
        }
      ],
      "source": [
        "## setup(1min)\n",
        "# torch 버전 다운. torch>=2.0 에선 colosalai가 동작안함\n",
        "!pip uninstall torch -y\n",
        "!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:{}\".format(torch.__version__))\n",
        "print(\"cuda version: {}\".format(torch.version.cuda))\n",
        "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))\n",
        "\n",
        "# for transformers, 최신버전은 에러발생\n",
        "!pip install transformers==4.35.2\n",
        "!pip install accelerate==0.24.1\n",
        "\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "\n",
        "# setup data\n",
        "!git clone https://github.com/airobotlab/KoChatGPT\n",
        "!mv KoChatGPT/data_kochatgpt .\n",
        "!mv KoChatGPT/img .\n",
        "\n",
        "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4KFYan9SsQJ"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
        "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
        "from copy import deepcopy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import copy\n",
        "import logging\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 경로 설정"
      ],
      "metadata": {
        "id": "pXVkBrvEKmxb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgOvAY68fBd7",
        "outputId": "b2936817-8557-414b-d2f5-473fe5f3e16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data_path_1_SFT='/content/drive/MyDrive/KoMo/Dataset/total_minus.csv', model_name='skt/kogpt2-base-v2', max_epochs=5, train_batch_size=8, output_dir='/content/drive/MyDrive/KoMo/SFT/SFT_0521')\n"
          ]
        }
      ],
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "DATA_PATH = '/content/drive/MyDrive/KoMo/Dataset/total_minus.csv'\n",
        "OUTPUT_PATH = '/content/drive/MyDrive/KoMo/SFT/SFT_0521'\n",
        "parser.add_argument('--data_path_1_SFT', type=str, default=DATA_PATH)\n",
        "parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--max_epochs', type=int, default=2)\n",
        "parser.add_argument('--train_batch_size', type=int, default=8)\n",
        "parser.add_argument('--output_dir', type=str, default=OUTPUT_PATH)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n",
        "args.max_epochs = 5\n",
        "\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wy7JlkgeCFL"
      },
      "source": [
        "# Finetune SFT(koAlpaca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TGzmiumsS4Cx",
        "outputId": "c031f441-44b8-4a82-bd69-31d0a85aea89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', ':)', 'l^o']\n"
          ]
        }
      ],
      "source": [
        "## test & load skt gpt2 kroean\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, pipeline\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "                                                    pad_token='<pad>', mask_token='<mask>')\n",
        "print(tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\"))\n",
        "# ['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', ':)', 'l^o']\n",
        "\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "text = '근육이 커지기 위해서는'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "# print(generated)\n",
        "\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    eos_token_id=375, # \\n\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")\n",
        "# generator(\n",
        "#     [\"0 : **는 게임 좋아하니\\n1 :\",\n",
        "#     \"0 : 어제 강남에서 살인사건 났대 ㅜㅜ 너무 무서워\\n1 : 헐 왜? 무슨 일 있었어?\\n0 : 사진보니까 막 피흘리는 사람있고 경찰들이 떠서 제압하고 난리도 아니었다던데??\\n1 :\",\n",
        "#     \"0 : 자기야 어제는 나한테 왜 그랬어?\\n1 : 뭔 일 있었어?\\n0 : 어떻게 나한테 말도 없이 그럴 수 있어? 나 진짜 실망했어\\n1 : \"],\n",
        "#     **generation_args\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOpkqknObfb9"
      },
      "outputs": [],
      "source": [
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"아래는 순화해야 할 욕설, 혐오 표현이 포함된 문장과 순화된 문장이 짝을 이루는 예제입니다.\\n\\n\"\n",
        "        \"욕설, 혐오 표현을 순화한 응답을 작성하세요. 이때 문장에 어울리는 이모지를 붙이세요.\\n\\n\"\n",
        "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n### Response(응답):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요. 이때 문장에 어울리는 이모지를 붙이세요.\\n\\n\"\n",
        "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B4i959nM5bx",
        "outputId": "e25efe3e-6b2f-4721-c49a-14635ce8414b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tokens added successfully.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AddedToken, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "emoji_tokens = [\n",
        "    \"😀\", \"😁\", \"😂\", \"🤣\", \"😃\", \"😄\", \"😅\", \"😆\", \"😉\", \"😊\", \"😋\", \"😎\", \"😍\", \"😘\", \"🥰\", \"😗\", \"😙\", \"😚\",\n",
        "    \"🙂\", \"🤗\", \"🤩\", \"🤔\", \"🤨\", \"😐\", \"😑\", \"😶\", \"🙄\", \"😏\", \"😣\", \"😥\", \"😮\", \"🤐\", \"😯\", \"😪\", \"😫\", \"🥱\",\n",
        "    \"😴\", \"😌\", \"😛\", \"😜\", \"😝\", \"🤤\", \"😒\", \"😓\", \"😔\", \"😕\", \"🙃\", \"🤑\", \"😲\", \"☹\", \"🙁\", \"😖\", \"😞\", \"😟\",\n",
        "    \"😤\", \"😢\", \"😭\", \"😦\", \"😧\", \"😨\", \"😩\", \"🤯\", \"😬\", \"😰\", \"😱\", \"🥵\", \"🥶\", \"😳\", \"🤪\", \"😵\", \"😡\", \"😠\",\n",
        "    \"🤬\", \"😷\", \"🤒\", \"🤕\", \"🤢\", \"🤮\", \"🤧\", \"😇\", \"🥳\", \"🥺\", \"🤠\", \"🤡\", \"🤥\", \"🤫\", \"🤭\", \"🧐\", \"🤓\", \"😈\",\n",
        "    \"👿\", \"👹\", \"👺\", \"💀\", \"👻\", \"👽\", \"👾\", \"🤖\",\n",
        "\n",
        "\n",
        "    \"💌\", \"🕳\", \"💣\", \"💎\", \"🔪\", \"🗡\", \"⚔\", \"🛡\", \"🚬\", \"⚰\", \"⚱\", \"🏺\", \"🔮\", \"📿\", \"💈\", \"⚗\", \"🔭\", \"🔬\",\n",
        "    \"🕯\", \"💡\", \"🔦\", \"🏮\", \"📔\", \"📕\", \"📖\", \"📗\", \"📘\", \"📙\", \"📚\", \"📓\", \"📒\", \"📃\", \"📜\", \"📄\", \"📰\",\n",
        "    \"🗞\", \"📑\", \"🔖\", \"🏷\", \"💰\", \"💴\", \"💵\", \"💶\", \"💷\", \"💸\", \"💳\", \"🧾\", \"💹\", \"✉\", \"📧\", \"📨\", \"📩\",\n",
        "    \"📤\", \"📥\", \"📦\", \"📫\", \"📪\", \"📬\", \"📭\", \"📮\", \"🗳\", \"✏\", \"✒\", \"🖋\", \"🖊\", \"🖌\", \"🖍\", \"📝\", \"💼\",\n",
        "    \"📁\", \"📂\", \"🗂\", \"📅\", \"📆\", \"🗒\", \"🗓\", \"📇\", \"📈\", \"📉\", \"📊\", \"📋\", \"📌\", \"📍\", \"📎\", \"🖇\", \"📏\",\n",
        "    \"📐\", \"✂\", \"🗃\", \"🗄\", \"🗑\", \"🔒\", \"🔓\", \"🔏\", \"🔐\", \"🔑\", \"🗝\", \"🔨\", \"🪓\", \"⛏\", \"⚒\", \"🛠\", \"🗡\", \"⚔\",\n",
        "    \"🔫\", \"🏹\", \"🛡\", \"🔧\", \"🔩\", \"⚙\", \"🗜\", \"⚖\", \"🦯\", \"🔗\", \"⛓\", \"🧰\", \"🧲\", \"🧪\", \"🧫\", \"🧬\", \"🔬\",\n",
        "    \"🔭\", \"📡\", \"💉\", \"💊\", \"🩸\", \"🩹\", \"🩺\", \"🚪\", \"🛏\", \"🛋\", \"🪑\", \"🚽\", \"🚿\", \"🛁\", \"🪒\", \"🧴\", \"🧷\",\n",
        "    \"🧹\", \"🧺\", \"🧻\", \"🧼\", \"🪣\", \"🧽\", \"🪤\", \"🪒\", \"🔑\", \"🗝\", \"🚪\", \"🛌\", \"🛋\", \"🛏\", \"🛋\", \"🪑\", \"🚽\",\n",
        "    \"🪣\", \"🛁\", \"🪞\", \"🪠\", \"🪤\", \"🪒\", \"🪥\", \"🛒\", \"🚬\", \"⚰\", \"⚱\", \"🪦\", \"🧿\", \"🪔\", \"🪒\"\n",
        "]\n",
        "\n",
        "\n",
        "added_emoji_tokens = [AddedToken(emoji, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True) for emoji in emoji_tokens]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    args.model_name,\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,\n",
        ")\n",
        "\n",
        "tokenizer.add_special_tokens(\n",
        "    {\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    }\n",
        ")\n",
        "\n",
        "tokenizer.add_tokens(added_emoji_tokens)\n",
        "\n",
        "for token in added_emoji_tokens:\n",
        "    assert tokenizer.convert_tokens_to_ids(token.content) != tokenizer.unk_token_id, f\"Token {token.content} was not added correctly.\"\n",
        "print(\"All tokens added successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eb6udwMNPu2",
        "outputId": "ec86f8f7-c60b-453e-ba88-ed6913a84b62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(51201, 768)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    args.model_name,\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,\n",
        ")\n",
        "tokenizer.add_special_tokens(\n",
        "    {\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    }\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b1b2UiafjfY"
      },
      "source": [
        "###SFT를 위한 데이터셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "PROnzF0iYFvc",
        "outputId": "dbacab82-d67d-4ea2-cb15-c026d03c5653"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 25628,\n  \"fields\": [\n    {\n      \"column\": \"Cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25625,\n        \"samples\": [\n          \"\\uaecc\\uc744 \\uc539\\uc740 \\uac78 \\ubabb \\ubd24\\ub294\\ub370 \\uc774\\uc720\\uac00 \\ube44\\uc2f8\\uc11c \\ubabb \\uc539\\uc740 \\uac70\\uc600\\uc5b4 \\u3137\\u3137...\",\n          \"\\ub098\\ub294 \\ud6c4\\uae30\\uac19\\uc740 \\uac70 \\ub2e4 \\ucc3e\\uc544\\ubcf4\\uace0 \\uac00\\ub294 \\uac70 \\uac19\\uc544 \\ud558\\ud558\",\n          \"\\ucf54\\ub85c\\ub098 \\uce58\\ub8cc\\uc81c \\ub098\\uc628 \\uac70 \\uc544\\ub2c8?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Emojis\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25623,\n        \"samples\": [\n          \"\\uc804\\uc138\\uc9d1 \\ub4e4\\uc5b4\\uac08 \\uac70\\uc57c 5\\uc5b5 \\uc815\\ub3c4\\uba74 \\ub418\\uaca0\\uc9c0? \\ud83d\\udcb0\",\n          \"\\ub9c8\\uc998 \\uafc0\\uc7bc\\uc774\\uc57c! \\uc2a4\\ud3ec \\ubd10\\ub3c4 \\uc7ac\\ubc0c\\uc5b4\\uc694. \\ud83d\\ude02\",\n          \"\\ub2e4\\uc774\\uc2a8 \\uc694\\uc998 \\uc800\\ub834\\ud574 \\ud83d\\udd25\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Profanity\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25620,\n        \"samples\": [\n          \"\\uc544 \\uc9c4\\uc9dc \\uc774\\ubc88\\uc8fc \\uc544\\ud615 \\uaf2d \\ubd10\\uc57c\\uaca0\\ub124 \\u314b\\u314b \\uc815\\ubbfc\\uc774 \\ub098\\uc628\\ub314\\uc790\\ub098 \\u3149\\u3149\\ubb34\\uc870\\uac74 \\ubcf8\\ubc29\\uc0ac\\uc218 \\uac01\\uc774\\ub2e4 \\u3147\\u3148? \\uc548\\ubcf4\\uba74 \\uadf8\\ub0e5 \\uc778\\uc0dd \\ud5db\\uc0b0\\uac70\\uc784 \\u314b\\u314b\\u314b\",\n          \"\\uc774 \\uac1c\\ub3fc\\uc9c0 \\uad6d\\uac00\\ub294 \\ubcc4\\uc5d0 \\ubcc4 \\uc4f0\\ub808\\uae30\\ub97c \\ub2e4 \\ucc59\\uae30\\ub178 \\u314b\\u314b\\u314b \\uc886\\uac19\\uc740 \\uc9d3\\uac70\\ub9ac\\ub9cc \\uace8\\ub77c\\ud558\\ub294 \\uc03c\\uc824\\ub77c \\uc0c8\\ub07c\\ub4e4\\uc774 \\ud310\\uc744 \\uce58\\ub294 \\uaf34\\uc774\\ub77c\\ub2c8\",\n          \"\\u314b\\u314b \\u314b\\u314b \\uace0\\ub370\\uae30 \\uc4f0\\ub294\\uac8c \\ub0ab\\ub2e4\\uace0? \\ub2c8 \\uba38\\ub9ac\\uc5d0 \\uace0\\ub370\\uae30\\ub85c \\uc9c0\\uc9c0\\uba74 \\uc5b4\\ub5bb\\uac8c \\ub420\\uc9c0 \\uc0c1\\uc0c1\\ub3c4 \\uc548\\ub418\\ub0d0? \\u314b\\u314b \\uadf8\\ub0e5 \\ub4dc\\ub77c\\uc774\\uae30\\ub098 \\uc4f0\\ub77c\\ub2c8\\uae4c \\u3145\\u3142\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ac87a277-403c-4b07-bbed-2edbefa4cf72\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cleaned</th>\n",
              "      <th>Emojis</th>\n",
              "      <th>Profanity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>와 그런 사람들 되게 많다더라.</td>\n",
              "      <td>와 그런 사람들 되게 많다더라. 😮</td>\n",
              "      <td>와 그딴 씹선비들 개많음 ㅋㅋㅋ 걍 니들이나 잘하세요~</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>근데 난 대학 안 가서 주변에 아저씨 뿐이라 몰라</td>\n",
              "      <td>근데 난 대학 안 가서 주변에 아저씨 🤷‍♂️ 뿐이라 몰라</td>\n",
              "      <td>대학 씹고 니 주변에 아재들밖에 없다고? 그거 니 인생 망한거 아님? ㅋㅋㅋ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>대학교를 가야 세상 다양한 주변 사람들 많이 사귀지 왜 안 갔어</td>\n",
              "      <td>대학교를 가야 세상 다양한 주변 사람들 많이 사귀지 🤔 왜 안 갔어</td>\n",
              "      <td>대학 안간게 뭐가 문제냐 씹선비야 ㅋㅋ 니가 대학가서 좆같은 인맥이나 쌓으면서 살아...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>난 대학교 졸업장이 필수가 되어버린 이 사회가 슬프다</td>\n",
              "      <td>난 대학교 졸업장이 필수가 되어버린 이 사회가 슬프다 😢</td>\n",
              "      <td>대학 졸업장 없으면 개돼지 취급 받는 이 사회 참 ㅈ같네 ㅉㅉ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>지금 고졸 무시하시는 건가요? 신고합니다</td>\n",
              "      <td>지금 고졸 😤 무시하시는 건가요? 신고합니다</td>\n",
              "      <td>지금 고졸 무시하는거 씨발? 신고 갈기노~</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac87a277-403c-4b07-bbed-2edbefa4cf72')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac87a277-403c-4b07-bbed-2edbefa4cf72 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac87a277-403c-4b07-bbed-2edbefa4cf72');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-691d5930-1772-4796-ae31-813bacde4817\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-691d5930-1772-4796-ae31-813bacde4817')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-691d5930-1772-4796-ae31-813bacde4817 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                               Cleaned                                 Emojis  \\\n",
              "0                    와 그런 사람들 되게 많다더라.                    와 그런 사람들 되게 많다더라. 😮   \n",
              "1          근데 난 대학 안 가서 주변에 아저씨 뿐이라 몰라       근데 난 대학 안 가서 주변에 아저씨 🤷‍♂️ 뿐이라 몰라   \n",
              "2  대학교를 가야 세상 다양한 주변 사람들 많이 사귀지 왜 안 갔어  대학교를 가야 세상 다양한 주변 사람들 많이 사귀지 🤔 왜 안 갔어   \n",
              "3        난 대학교 졸업장이 필수가 되어버린 이 사회가 슬프다        난 대학교 졸업장이 필수가 되어버린 이 사회가 슬프다 😢   \n",
              "4               지금 고졸 무시하시는 건가요? 신고합니다               지금 고졸 😤 무시하시는 건가요? 신고합니다   \n",
              "\n",
              "                                           Profanity  \n",
              "0                     와 그딴 씹선비들 개많음 ㅋㅋㅋ 걍 니들이나 잘하세요~  \n",
              "1         대학 씹고 니 주변에 아재들밖에 없다고? 그거 니 인생 망한거 아님? ㅋㅋㅋ  \n",
              "2  대학 안간게 뭐가 문제냐 씹선비야 ㅋㅋ 니가 대학가서 좆같은 인맥이나 쌓으면서 살아...  \n",
              "3                 대학 졸업장 없으면 개돼지 취급 받는 이 사회 참 ㅈ같네 ㅉㅉ  \n",
              "4                            지금 고졸 무시하는거 씨발? 신고 갈기노~  "
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(DATA_PATH)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQCifnvfz7Kt",
        "outputId": "889d1893-d3bc-4a88-9826-d7ac0da7baa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25628 entries, 0 to 25627\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Cleaned    25628 non-null  object\n",
            " 1   Emojis     25628 non-null  object\n",
            " 2   Profanity  25628 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 600.8+ KB\n"
          ]
        }
      ],
      "source": [
        "# train_len = 10000\n",
        "train_len = len(data)\n",
        "data = data[:train_len]\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVUXxmUa1S9j"
      },
      "outputs": [],
      "source": [
        "# DATA_PATH_SFT = '/content/drive/MyDrive/KoMo/Dataset/data1.csv' # 10000개로 학습\n",
        "DATA_PATH_SFT = '/content/drive/MyDrive/KoMo/Dataset/total_minus.csv' # 25628개로 학습\n",
        "# data = pd.read_csv(DATA_PATH_SFT)\n",
        "# data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "SlegPuuEwAZo",
        "outputId": "2820aeda-8d7d-4de9-830c-ef7029cae22a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DATA_PATH_SFT' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1603feb4cb88>\u001b[0m in \u001b[0;36m<cell line: 122>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m# Make sure to define `args`, `tokenizer`, and `data_path_1_SFT` properly in your actual implementation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSFT_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH_SFT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# eval은 안함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorForSupervisedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DATA_PATH_SFT' is not defined"
          ]
        }
      ],
      "source": [
        "from typing import Optional, Dict, Sequence\n",
        "from torch.utils.data import Dataset\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import copy\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "IGNORE_INDEX = -100  # Define IGNORE_INDEX if not defined elsewhere\n",
        "\n",
        "class SFT_dataset(Dataset):\n",
        "    '''SFT dataset by wygo'''\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
        "        super(SFT_dataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "\n",
        "        ## format\n",
        "        pattern_instruction = 'prompt'  # 순화를 지시하는 명령 (프롬프트)\n",
        "        pattern_input = 'input'  # 순화시킬 데이터\n",
        "        pattern_output = 'completion'  # 순화된 output\n",
        "\n",
        "        ############################################################\n",
        "        ## load dataset\n",
        "        list_data_csv = pd.read_csv(data_path, encoding='utf-8')\n",
        "        profanity_data = list_data_csv['Profanity']\n",
        "        cleaned_data = list_data_csv['Emojis']\n",
        "        if True:\n",
        "            print(profanity_data[0])\n",
        "\n",
        "        ############################################################\n",
        "        ## 데이터셋 만들기, source와 target\n",
        "\n",
        "        # 입력\n",
        "        sources = []\n",
        "        for example in profanity_data:\n",
        "            if example != \"\":\n",
        "                prompt = f\"\"\"\n",
        "Input에 욕설, 혐오 표현이 있다면 이를 찾아 예쁜 말로 순화해줘.\\n\\n\n",
        "Input(순화할 문장):\\n{example}\\n\\nOutput(순화한 문장):\"\"\"\n",
        "            else:\n",
        "                print(\"Error\")\n",
        "            sources.append(prompt)\n",
        "\n",
        "        # 출력\n",
        "        targets = []\n",
        "        for example in cleaned_data:\n",
        "            targets.append(f\"{example}{tokenizer.eos_token}\" if isinstance(example, str) else f\"{tokenizer.eos_token}\")\n",
        "        print(\"Example - sources[0]\", sources[0])\n",
        "        print(\"Example - tragets[0]\", targets[0])\n",
        "        print(\"Example - sources[-1]\", sources[-1])\n",
        "        print(\"Example - tragets[-1]\", targets[-1])\n",
        "\n",
        "        if verbose:\n",
        "            idx = 0\n",
        "            print(sources[idx])\n",
        "            print(targets[idx])\n",
        "            print(\"Tokenizing inputs... This may take some time...\")\n",
        "\n",
        "        examples = [s + t for s, t in zip(sources, targets)]\n",
        "\n",
        "        # source data tokenized\n",
        "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
        "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
        "\n",
        "        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n",
        "        input_ids = examples_tokenized[\"input_ids\"]\n",
        "        labels = copy.deepcopy(input_ids)\n",
        "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n",
        "\n",
        "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
        "\n",
        "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "        \"\"\"Tokenize a list of strings.\"\"\"\n",
        "        tokenized_list = [\n",
        "            tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"longest\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "            )\n",
        "            for text in strings\n",
        "        ]\n",
        "        input_ids = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "        input_ids_lens = [tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            input_ids_lens=input_ids_lens,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "# Example usage\n",
        "# Make sure to define `args`, `tokenizer`, and `data_path_1_SFT` properly in your actual implementation.\n",
        "train_dataset = SFT_dataset(data_path=DATA_PATH_SFT, tokenizer=tokenizer)\n",
        "eval_dataset = None  # eval은 안함\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "# Check the first sample\n",
        "print(\"Dataset len\", len(train_dataset))\n",
        "print('input : %s' % train_dataset.input_ids[0])\n",
        "print('output: %s' % train_dataset.labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XG6kSJoILB1"
      },
      "source": [
        "### 5 Epoch 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "fF3GJA6dgJsP",
        "outputId": "7b290ed5-5da2-4f9b-e432-4c597a2594af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'data_collator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2c39260c768d>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_collator' is not defined"
          ]
        }
      ],
      "source": [
        "## 학습 (10min)\n",
        "# training_args 수정 가능: https://github.com/Beomi/KoAlpaca/blob/main/train.sh 참고\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "\"\"\"\n",
        "# 모델 추가 학습할 때 주석 해제 후 model load\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/KoMo/SFT/0521_25628_10epoch/checkpoint-15000'   # 10 Epoch , Train data -all\n",
        "model = AutoModelForCausalLM.from_pretrained(CHECKPOINT_PATH)\n",
        "\"\"\"\n",
        "\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/KoMo/SFT/0521_25628_10epoch'\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR, #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=10, # number of training epochs\n",
        "    per_device_train_batch_size=16, # batch size for training\n",
        "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
        "    eval_steps = 3, # Number of update steps between two evaluations.\n",
        "    save_steps=2500, # after # steps model is saved\n",
        "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    run_name=\"0521_0320\",\n",
        "    )\n",
        "trainer = Trainer(\n",
        "    model=model.to(device),\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owz-cbylyEL4"
      },
      "source": [
        "### SFT Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "collapsed": true,
        "id": "-ENEo96nyGcs",
        "outputId": "1174c285-318b-448d-edbe-394047cdeaa3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2be69f55521e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# CHECKPOINT_PATH = '/content/drive/MyDrive/KoMo/SFT/0521_25628_10epoch/checkpoint-12500'   # 10 Epoch , Train data -all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# CHECKPOINT_PATH = '/content/drive/MyDrive/KoMo/SFT/0521_25628_10epoch/checkpoint-10000'   # 15 Epoch , Train data -all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ],
      "source": [
        "CHECKPOINT_PATH = '/content/drive/MyDrive/KoMo/SFT/0521_2/checkpoint-2500'   # 10 Epoch , Train data 10000\n",
        "model = AutoModelForCausalLM.from_pretrained(CHECKPOINT_PATH)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "def infer(input_text: str, model, tokenizer, max_length: int = 512):\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            bos_token_id=tokenizer.bos_token_id,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlM4QrNOakkJ"
      },
      "outputs": [],
      "source": [
        "sample_list = ['에휴 씨발 물가 좆같이 올라서 먹을 수 있는게 없노', '조선족 개새끼들 한국땅에서 못살게 존나 다 내쫓아야한다이기야', '개쌉레알이노 ㅋㅋㅋㅋ 웹툰보이가 그린웹툰들 몰입력 ㅆㅎㅌㅊ에 내용 ㅆㅎㅌㅊ 반면 웹툰작가꺼는 나 ㅆㅅㅌㅊ에 잼임', 'ㅋㅋㅋㅋ 상상속에서 맘충이랑 싸움했ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ일베에서나 키보드로 욕이나 하지 현실에선 말한마디나 제대로 하노?', '요즘 유흥에도 베트남년들 개많더라 오피에도 베트남년들 태국년들있고 대부분 국걸로 입국해서 화재들 뒷통수치고 가출할거겠지만 ㅋㅋㅋㅋㅋ']\n",
        "sample_list += ['아 미친 존나 배고파', '교수 과제 왜 이따구로 냄?', 'ㅅㅂ 집가고 싶다']\n",
        "sample_list += ['ㅈㄴ 게이같노 ㅋㅋ', '지랄하지말고 묻는 말에 대답이나해 이기야', '저 미친 ㄴ ㅋㅋㅋㅋㅋ', '한남 수준 실화노 ㅋㅋ']\n",
        "sample_list += ['한남 수준 ㅉㅉ', '말하는 꼬라지만봐도 홍어 냄새 나네 라도냐?', '개슬람 다 불태워버리고 싶다', '시발 ㅈ같네']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uI_y1tscELIQ",
        "outputId": "1efb9046-cd81-4825-c7e5-64a7430e8937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 에휴 씨발 물가 좆같이 올라서 먹을 수 있는게 없노\n",
            "\n",
            "    Output(순화한 문장):\n",
            "     물가 올라서 먹을 수 있는 게 없더라고\n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 조선족 개새끼들 한국땅에서 못살게 존나 다 내쫓아야한다이기야\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ조선족도 못 살게 됨 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 개쌉레알이노 ᄏᄏᄏᄏ 웹툰보이가 그린웹툰들 몰입력 ᄊᄒᄐᄎ에 내용 ᄊᄒᄐᄎ 반면 웹툰작가꺼는 나 ᄊᄉᄐᄎ에 잼임\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ 웹툰이 그린 웹툰들 밀면 좋더라 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): ᄏᄏᄏᄏ 상상속에서 맘충이랑 싸움했ᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏ일베에서나 키보드로 욕이나 하지 현실에선 말한마디나 제대로 하노?\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ 상상속에서  싸움다녔다 ᄏᄏ \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 요즘 유흥에도 베트남년들 개많더라 오피에도 베트남년들 태국년들있고 대부분 국걸로 입국해서 화재들 뒷통수치고 가출할거겠지만 ᄏᄏᄏᄏᄏ\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ 요즘밍에도 베트남이랑 중국  많이 가는 거 같아 ᄏᄏ\n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 아 미친 존나 배고파\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    헉 진짜 아빠 너무 파박 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 교수 과제 왜 이따구로 냄?\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ 교수 과제는 왜 이렇게 없어? \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): ᄉᄇ 집가고 싶다\n",
            "\n",
            "    Output(순화한 문장):\n",
            "     집 가고 싶다 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): ᄌᄂ 게이같노 ᄏᄏ\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ 게이좋네용 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 지랄하지말고 묻는 말에 대답이나해 이기야\n",
            "\n",
            "    Output(순화한 문장):\n",
            "     정말? 짓고 답하는 말면 그냥 어울리는 거 같아 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 저 미친 ᄂ ᄏᄏᄏᄏᄏ\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    헉  저 미친 거다 ᄏᄏ\n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 한남 수준 실화노 ᄏᄏ\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ  니 수준이 실화냐 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 한남 수준 ᄍᄍ\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ 완전 넘 넘 잼아 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 말하는 꼬라지만봐도 홍어 냄새 나네 라도냐?\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ  말 할 줄 알아도 홍어 냄새나?\n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 개슬람 다 불태워버리고 싶다\n",
            "\n",
            "    Output(순화한 문장):\n",
            "     개슬람 다 불태워버리고 싶다 \n",
            "Output: \n",
            "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\n",
            "\n",
            "\n",
            "    Input(순화할 문장): 시발 ᄌ같네\n",
            "\n",
            "    Output(순화한 문장):\n",
            "    ᄏᄏ \n"
          ]
        }
      ],
      "source": [
        "for sample in sample_list:\n",
        "    input_text = f\"\"\"\n",
        "    Input 문장의 욕설, 혐오 표현을 순화한 응답을 작성하세요.\\n\\n\n",
        "    Input(순화할 문장): {sample}\\n\n",
        "    Output(순화한 문장):\n",
        "    \"\"\"\n",
        "    output_text = infer(input_text, model, tokenizer)\n",
        "    # print(f\"Input: {input_text}\")\n",
        "    print(f\"Output: {output_text}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "uHESbi8QMpgt",
        "OXql23cBd-j-",
        "8rOGnv2ISrnE",
        "-Wy7JlkgeCFL",
        "owz-cbylyEL4",
        "BjH70tUBGHu-",
        "de90QcYkP9sO",
        "5QSuRZMD8PSN",
        "OhggrZ_nRbK-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
