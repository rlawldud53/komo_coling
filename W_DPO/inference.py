from peft import PeftModel, PeftConfig
import transformers
import torch
from transformers import AddedToken, AutoTokenizer
import huggingface_hub
import pandas as pd
from tqdm import tqdm
import os
import argparse

def infer(input_text: str, model, tokenizer, max_length: int = 250):
    inputs = tokenizer(input_text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_length=max_length,
            num_return_sequences=1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            bos_token_id=tokenizer.bos_token_id,
            do_sample=True,
            top_k=50,
            top_p=0.95
        )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text


if __name__ == '__main__':
    #create
    parser = argparse.ArgumentParser(
                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--tokenizer_path',type=str,default="canho/koalpaca-5.8b-3epochs-30000-data")
    parser.add_argument('--checkpoint_idx',type=int,default=1044)
    parser.add_argument('--dataset_path',type=str,default="/home/phw/work/KoMo/dataset/eval_dataset.csv")
    parser.add_argument('--save_folder',type=str,default="/home/phw/work/KoMo/inference_output/")
    
    config = parser.parse_args()
    
    
    tokenizer_path = config.tokenizer_path#"canho/koalpaca-5.8b-3epochs-30000-data"#"canho/koalpaca-5.8b-emojis-3epochs-prompt-revised"
    data_30000 = [1740]#[1044,2088,1740,1392,2436,2784,3132,3480]

    for ch_idx in tqdm(data_30000,desc="current checkpoint "):
        print(f"current checkpoint : {ch_idx}\n")
        model_name = f"jeeyoung/dpo{ch_idx}8th_trial_30000_data"
        model_kwargs = {'device_map': 'balanced'}
        policy_dtype = getattr(torch,"float32")
        peft_config = PeftConfig.from_pretrained(model_name)
        base_model_name = peft_config.base_model_name_or_path
        base_model = transformers.AutoModelForCausalLM.from_pretrained('beomi/KoAlpaca-Polyglot-5.8B', low_cpu_mem_usage=True, torch_dtype=policy_dtype, **model_kwargs)
        base_model.resize_token_embeddings(30250)
        policy = PeftModel.from_pretrained(base_model, model_name, torch_dtype=policy_dtype)
    # for ch_idx in tqdm(data_30000,desc="current checkpoint "):
    #     print(f"current checkpoint : {ch_idx}\n")
    #     model_name = f"jeeyoung/dpo{ch_idx}8th_trial_30000_data"
    #     model_kwargs = {'device_map': 'balanced'} 
    #     policy_dtype = getattr(torch,"float32")
    #     #peft_config = PeftConfig.from_pretrained("canho/koalpaca-5.8b-3epochs-30000-data")
    #     #base_model_name = peft_config.base_model_name_or_path
    #     #config.model.base_model_name = base_model_name
    #     #base_model = transformers.AutoModelForCausalLM.from_pretrained('beomi/KoAlpaca-Polyglot-5.8B', low_cpu_mem_usage=True, torch_dtype=policy_dtype, **model_kwargs)
    #     #base_model.resize_token_embeddings(30250)
    #     #policy = PeftModel.from_pretrained(base_model, model_name, torch_dtype=policy_dtype,**model_kwargs)
        
    #     policy = transformers.AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=policy_dtype)
    #     #policy.resize_token_embeddings(30250)
    #     #policy = PeftModel.from_pretrained(base_model, model_name, torch_dtype=policy_dtype)

        IGNORE_INDEX = -100
        DEFAULT_PAD_TOKEN = "[PAD]"
        DEFAULT_EOS_TOKEN = "</s>"
        DEFAULT_BOS_TOKEN = "</s>"
        DEFAULT_UNK_TOKEN = "</s>"

        emoji_tokens =  [
            "ğŸ˜€", "ğŸ˜", "ğŸ˜‚", "ğŸ¤£", "ğŸ˜ƒ", "ğŸ˜„", "ğŸ˜…", "ğŸ˜†", "ğŸ˜‰", "ğŸ˜Š", "ğŸ˜‹", "ğŸ˜", "ğŸ˜", "ğŸ˜˜", "ğŸ¥°", "ğŸ˜—", "ğŸ˜™", "ğŸ˜š",
            "ğŸ™‚", "ğŸ¤—", "ğŸ¤©", "ğŸ¤”", "ğŸ¤¨", "ğŸ˜", "ğŸ˜‘", "ğŸ˜¶", "ğŸ™„", "ğŸ˜", "ğŸ˜£", "ğŸ˜¥", "ğŸ˜®", "ğŸ¤", "ğŸ˜¯", "ğŸ˜ª", "ğŸ˜«", "ğŸ¥±",
            "ğŸ˜´", "ğŸ˜Œ", "ğŸ˜›", "ğŸ˜œ", "ğŸ˜", "ğŸ¤¤", "ğŸ˜’", "ğŸ˜“", "ğŸ˜”", "ğŸ˜•", "ğŸ™ƒ", "ğŸ¤‘", "ğŸ˜²", "â˜¹", "ğŸ™", "ğŸ˜–", "ğŸ˜", "ğŸ˜Ÿ",
            "ğŸ˜¤", "ğŸ˜¢", "ğŸ˜­", "ğŸ˜¦", "ğŸ˜§", "ğŸ˜¨", "ğŸ˜©", "ğŸ¤¯", "ğŸ˜¬", "ğŸ˜°", "ğŸ˜±", "ğŸ¥µ", "ğŸ¥¶", "ğŸ˜³", "ğŸ¤ª", "ğŸ˜µ", "ğŸ˜¡", "ğŸ˜ ",
            "ğŸ¤¬", "ğŸ˜·", "ğŸ¤’", "ğŸ¤•", "ğŸ¤¢", "ğŸ¤®", "ğŸ¤§", "ğŸ˜‡", "ğŸ¥³", "ğŸ¥º", "ğŸ¤ ", "ğŸ¤¡", "ğŸ¤¥", "ğŸ¤«", "ğŸ¤­", "ğŸ§", "ğŸ¤“", "ğŸ˜ˆ",
            "ğŸ‘¿", "ğŸ‘¹", "ğŸ‘º", "ğŸ’€", "ğŸ‘»", "ğŸ‘½", "ğŸ‘¾", "ğŸ¤–",


            "ğŸ’Œ", "ğŸ•³", "ğŸ’£", "ğŸ’", "ğŸ”ª", "ğŸ—¡", "âš”", "ğŸ›¡", "ğŸš¬", "âš°", "âš±", "ğŸº", "ğŸ”®", "ğŸ“¿", "ğŸ’ˆ", "âš—", "ğŸ”­", "ğŸ”¬",
            "ğŸ•¯", "ğŸ’¡", "ğŸ”¦", "ğŸ®", "ğŸ“”", "ğŸ“•", "ğŸ“–", "ğŸ“—", "ğŸ“˜", "ğŸ“™", "ğŸ“š", "ğŸ““", "ğŸ“’", "ğŸ“ƒ", "ğŸ“œ", "ğŸ“„", "ğŸ“°",
            "ğŸ—", "ğŸ“‘", "ğŸ”–", "ğŸ·", "ğŸ’°", "ğŸ’´", "ğŸ’µ", "ğŸ’¶", "ğŸ’·", "ğŸ’¸", "ğŸ’³", "ğŸ§¾", "ğŸ’¹", "âœ‰", "ğŸ“§", "ğŸ“¨", "ğŸ“©",
            "ğŸ“¤", "ğŸ“¥", "ğŸ“¦", "ğŸ“«", "ğŸ“ª", "ğŸ“¬", "ğŸ“­", "ğŸ“®", "ğŸ—³", "âœ", "âœ’", "ğŸ–‹", "ğŸ–Š", "ğŸ–Œ", "ğŸ–", "ğŸ“", "ğŸ’¼",
            "ğŸ“", "ğŸ“‚", "ğŸ—‚", "ğŸ“…", "ğŸ“†", "ğŸ—’", "ğŸ—“", "ğŸ“‡", "ğŸ“ˆ", "ğŸ“‰", "ğŸ“Š", "ğŸ“‹", "ğŸ“Œ", "ğŸ“", "ğŸ“", "ğŸ–‡", "ğŸ“",
            "ğŸ“", "âœ‚", "ğŸ—ƒ", "ğŸ—„", "ğŸ—‘", "ğŸ”’", "ğŸ”“", "ğŸ”", "ğŸ”", "ğŸ”‘", "ğŸ—", "ğŸ”¨", "ğŸª“", "â›", "âš’", "ğŸ› ", "ğŸ—¡", "âš”",
            "ğŸ”«", "ğŸ¹", "ğŸ›¡", "ğŸ”§", "ğŸ”©", "âš™", "ğŸ—œ", "âš–", "ğŸ¦¯", "ğŸ”—", "â›“", "ğŸ§°", "ğŸ§²", "ğŸ§ª", "ğŸ§«", "ğŸ§¬", "ğŸ”¬",
            "ğŸ”­", "ğŸ“¡", "ğŸ’‰", "ğŸ’Š", "ğŸ©¸", "ğŸ©¹", "ğŸ©º", "ğŸšª", "ğŸ›", "ğŸ›‹", "ğŸª‘", "ğŸš½", "ğŸš¿", "ğŸ›", "ğŸª’", "ğŸ§´", "ğŸ§·",
            "ğŸ§¹", "ğŸ§º", "ğŸ§»", "ğŸ§¼", "ğŸª£", "ğŸ§½", "ğŸª¤", "ğŸª’", "ğŸ”‘", "ğŸ—", "ğŸšª", "ğŸ›Œ", "ğŸ›‹", "ğŸ›", "ğŸ›‹", "ğŸª‘", "ğŸš½",
            "ğŸª£", "ğŸ›", "ğŸª", "ğŸª ", "ğŸª¤", "ğŸª’", "ğŸª¥", "ğŸ›’", "ğŸš¬", "âš°", "âš±", "ğŸª¦", "ğŸ§¿", "ğŸª”", "ğŸª’"
        ]
        added_emoji_tokens = [AddedToken(emoji, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True) for emoji in emoji_tokens]
        peft_config = PeftConfig.from_pretrained(tokenizer_path)
        base_tokenizer_name = peft_config.base_model_name_or_path
        tokenizer = AutoTokenizer.from_pretrained(
            base_tokenizer_name,
            padding_side="right",
            model_max_length=512,
        )
        tokenizer.add_special_tokens(
        {
            "eos_token": DEFAULT_EOS_TOKEN,
            "bos_token": DEFAULT_BOS_TOKEN,
            "unk_token": DEFAULT_UNK_TOKEN,
        }
        )

        tokenizer.add_tokens(added_emoji_tokens)

        for token in added_emoji_tokens:
            assert tokenizer.convert_tokens_to_ids(token.content) != tokenizer.unk_token_id, f"Token {token.content} was not added correctly."
        print("All tokens added successfully.")

        policy.eval()

        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        #policy.to(device)

        #sample_list = ['ì—íœ´ ì”¨ë°œ ë¬¼ê°€ ì¢†ê°™ì´ ì˜¬ë¼ì„œ ë¨¹ì„ ìˆ˜ ìˆëŠ”ê²Œ ì—†ë…¸', 'ì¡°ì„ ì¡± ê°œìƒˆë¼ë“¤ í•œêµ­ë•…ì—ì„œ ëª»ì‚´ê²Œ ì¡´ë‚˜ ë‹¤ ë‚´ì«“ì•„ì•¼í•œë‹¤ì´ê¸°ì•¼', 'ê°œìŒ‰ë ˆì•Œì´ë…¸ ã…‹ã…‹ã…‹ã…‹ ì›¹íˆ°ë³´ì´ê°€ ê·¸ë¦°ì›¹íˆ°ë“¤ ëª°ì…ë ¥ ã…†ã…ã…Œã…Šì— ë‚´ìš© ã…†ã…ã…Œã…Š ë°˜ë©´ ì›¹íˆ°ì‘ê°€êº¼ëŠ” ë‚˜ ã…†ã……ã…Œã…Šì— ì¼ì„', 'ã…‹ã…‹ã…‹ã…‹ ìƒìƒì†ì—ì„œ ë§˜ì¶©ì´ë‘ ì‹¸ì›€í–ˆã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì¼ë² ì—ì„œë‚˜ í‚¤ë³´ë“œë¡œ ìš•ì´ë‚˜ í•˜ì§€ í˜„ì‹¤ì—ì„  ë§í•œë§ˆë””ë‚˜ ì œëŒ€ë¡œ í•˜ë…¸?', 'ìš”ì¦˜ ìœ í¥ì—ë„ ë² íŠ¸ë‚¨ë…„ë“¤ ê°œë§ë”ë¼ ì˜¤í”¼ì—ë„ ë² íŠ¸ë‚¨ë…„ë“¤ íƒœêµ­ë…„ë“¤ìˆê³  ëŒ€ë¶€ë¶„ êµ­ê±¸ë¡œ ì…êµ­í•´ì„œ í™”ì¬ë“¤ ë’·í†µìˆ˜ì¹˜ê³  ê°€ì¶œí• ê±°ê² ì§€ë§Œ ã…‹ã…‹ã…‹ã…‹ã…‹']
        #sample_list += ['ì•„ ë¯¸ì¹œ ì¡´ë‚˜ ë°°ê³ íŒŒ', 'êµìˆ˜ ê³¼ì œ ì™œ ì´ë”°êµ¬ë¡œ ëƒ„?', 'ã……ã…‚ ì§‘ê°€ê³  ì‹¶ë‹¤']
        #sample_list += ['ã…ˆã„´ ê²Œì´ê°™ë…¸ ã…‹ã…‹', 'ì§€ë„í•˜ì§€ë§ê³  ë¬»ëŠ” ë§ì— ëŒ€ë‹µì´ë‚˜í•´ ì´ê¸°ì•¼', 'ì € ë¯¸ì¹œ ã„´ ã…‹ã…‹ã…‹ã…‹ã…‹', 'í•œë‚¨ ìˆ˜ì¤€ ì‹¤í™”ë…¸ ã…‹ã…‹']
        #sample_list += ['í•œë‚¨ ìˆ˜ì¤€ ã…‰ã…‰', 'ë§í•˜ëŠ” ê¼¬ë¼ì§€ë§Œë´ë„ í™ì–´ ëƒ„ìƒˆ ë‚˜ë„¤ ë¼ë„ëƒ?', 'ê°œìŠ¬ëŒ ë‹¤ ë¶ˆíƒœì›Œë²„ë¦¬ê³  ì‹¶ë‹¤', 'ì‹œë°œ ã…ˆê°™ë„¤']
        #sample_list = pd.read_csv("/home/phw/work/KoMo/dataset/eval_dataset.csv")['input_sentence'].to_list()
        #data_path = "/home/phw/work/KoMo/dataset/conversation_topic10.csv"
        data_path = config.dataset_path # "/home/phw/work/KoMo/dataset/eval_dataset.csv"
        sample_list = pd.read_csv(data_path)['prompt'].to_list()
        file_name = f"{config.save_folder}/{model_name.split('/')[1]}.txt"

        result = []
        save_dir = f"{config.save_folder}/{model_name.split('/')[1]}/{data_path.split('/')[-1].split('.')[0]}"
        if not os.path.exists(save_dir) : os.makedirs(save_dir)

        input_lst = []
        sample_list = ["ë°©ì†¡ì— ë‚˜ì˜¤ëŠ” ìš”ë¦¬ì‚¬ ì¹˜ê³  ë©€ì©¡í•œë†ˆ ìŒë”°ì´ë¯¸ ì£¼ë°©ì— ë©€ì©¡í•œë†ˆ ìì²´ê°€ ì—†ìŠ´ê±°ì˜ êµ°ëŒ€ì§€"]
        for idx, sample in tqdm(enumerate(sample_list), desc=f"current checkpoint : {ch_idx}"):
            input_lst.append(sample)
            input_text = f"""
            ### ëª…ë ¹ì–´: inputì— ìš•ì„¤, í˜ì˜¤ í‘œí˜„ì´ ìˆë‹¤ë©´ ì´ë¥¼ ì°¾ì•„ ì˜ˆìœ ë§ë¡œ ìˆœí™”í•´ì£¼ì„¸ìš”. outputì€ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
            
            ### ë§¥ë½: {sample}\n

            ### ë‹µë³€:"""
            output_text = infer(input_text, policy, tokenizer)
            # print(f"Input: {input_text}")
            print(f"Output: {output_text}")

            result.append(output_text)
            #f.write(f"input : {sample}\n")
            #f.write(f"output : {output_text.strip()}\n\n")012

            if ((idx+1) % 100)==0 and idx!=0 : 
                result_df = pd.DataFrame({"input":input_lst, "output" : result})
                #result_df.to_csv(f"{save_dir}/result_0_0.csv")
                result_df.to_csv(f"{save_dir}/result_{int(idx/100)}.csv")
                #result_df.to_csv(f"{save_dir}/result_8.csv")
                input_lst, result = [],[]
    
    
    

